<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/build/_assets/app-BHPM2GEO.css"/><link rel="stylesheet" href="https://rsms.me/inter/inter.css"/><title>SHINOBI</title><script src="/babylon/babylon.js"></script><script src="/babylon/babylonjs.loaders.min.js"></script><script src="/babylon/babylon.gui.min.js"></script><script src="https://code.jquery.com/pep/0.4.3/pep.js"></script></head><body style="overscroll-behavior:none"><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0SNPJ02ZX8"></script><script async="" id="gtag-init">
								window.dataLayer = window.dataLayer || [];
								function gtag(){dataLayer.push(arguments);}
								gtag('js', new Date());

								gtag('config', 'G-0SNPJ02ZX8', {
								page_path: window.location.pathname,
								});
							</script><div class="flex w-full flex-col bg-white dark:bg-black"><div class="mx-auto w-full max-w-4xl px-4 py-12 pb-10 md:px-8"><div class="w-full flex flex-col justify-center items-center space-y-5 pb-10"><div class="w-full flex flex-col justify-center items-center space-y-5"><h1 class="text-primary font-sans text-3xl font-bold xl:text-5xl text-center">SHINOBI</h1><h2 class="text-primary font-sans text-xl font-medium xl:text-2xl text-center break-words px-2 max-w-[90%]">Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild</h2></div><div class="w-full flex flex-wrap justify-center items-center gap-5"><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Andreas Engelhardt</a><a href="https://amitraj93.github.io/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Amit Raj</a><a href="https://markboss.me/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Mark Boss</a><a href="https://cs.stanford.edu/~yzzhang/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Yunzhi Zhang</a><a href="https://abhishekkar.info/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Abhishek Kar</a><a href="https://people.csail.mit.edu/yzli/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Yuanzhen Li</a><a href="https://deqings.github.io/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Deqing Sun</a><a href="https://jonbarron.info/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Jonathan T. Barron</a><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Hendrik P.A. Lensch</a><a href="https://varunjampani.github.io/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center">Varun Jampani</a></div><div class="w-full flex flex-row justify-center items-center space-x-10"><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/" class="text-secondary font-sans cursor-pointer hover:text-primary"><img src="images/uni_logo.png" alt="University of Tübingen" class="w-auto h-20"/></a><a href="https://research.google/research-areas/machine-perception/" class="text-secondary font-sans cursor-pointer hover:text-primary"><img src="images/google_logo.png" alt="Google Research" class="w-auto h-20"/></a></div><div class="w-full flex flex-wrap justify-center items-center gap-5"><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80"><img src="/icons/arxiv.svg"/><span class="text-m  text-white">Paper</span></button><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80"><img src="/icons/pdf.svg"/><span class="text-m  text-white">Supplementary</span></button><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80"><img src="/icons/github.svg"/><span class="text-m  text-white">Code (coming soon)</span></button><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80"><img src="/icons/youtube.svg"/><span class="text-m  text-white">Video</span></button><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80"><img src="/icons/results.svg"/><span class="text-m  text-white">Results</span></button><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80"><img src="/icons/bibtex.svg"/><span class="text-m  text-white">Cite</span></button></div></div><div class="space-y-3 pt-16"><p class="text-justify block  font-normal "></p> <div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:0"><video autoplay="" loop="" playsinline="" muted="" class="" style="object-fit:contain;border-radius:0;transform:scale(undefined)"><source src="/videos/shinobi_teaser.mp4" type="video/mp4" data-src="/videos/shinobi_teaser.mp4"/></video></div><div class="prose mt-3"><p class="text-justify block font-normal "><strong>SHINOBI</strong> — <span>reconstructs shape, illumination and materials from in-the-wild image collections.<!-- -->   </span></p></div></div><p class="text-justify block font-normal "><span>We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object&#x27;s shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc.<!-- -->   </span></p></div><div class="space-y-10 pt-10 text-secondary"><div class="space-y-8"></div></div><div class="space-y-3 pt-16"><h1 class="text-primary font-sans text-xl font-bold">Overview</h1><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal "><span>We present a category-agnostic technique to jointly reconstruct 3D shape and material properties of objects from unconstrained in-the-wild image collections. This data regime poses multiple challenges as images are captured in different environments using a variety of devices resulting in varying backgrounds, illumination, camera poses, and intrinsics. Conventional structure-from-motion techniques like COLMAP fail to reconstruct image collections under these challenging conditions. Recent methods like <!-- -->   </span><a href="https://markboss.me/publication/2022-samurai/" class="text-primary underline cursor-pointer hover:text-secondary">SAMURAI [2]</a><span> and <!-- -->   </span><a href="https://jasonyzhang.com/ners/" class="text-primary underline cursor-pointer hover:text-secondary">NeRS [3]</a><span> can be initialized from very coarse view directions but still yield low quality reconstructions for many challenging scenes. Additionally, optimization takes more than 12 hours in the case of SAMURAI. In contrast, we propose a pipeline based on <!-- -->   </span><a href="https://nvlabs.github.io/instant-ngp/" class="text-primary underline cursor-pointer hover:text-secondary">multiresolution hash grids [4]</a><span> which allows us to process more rays in a shorter time. Using this advantage we are able to improve reconstruction quality while still keeping a competitive optimization time (~4 hours). However, naive integration of multi-resolution hash grids is not well suited to camera pose estimation due to discontinuities in the gradients with respect to the input positions. We propose several components that work together to stabilize the camera pose optimization and encourage sharp features:<!-- -->   </span></p><ul class="list-disc list-outside ml-5 text-justify block font-normal "><li><span>Hybrid Multiresolution Hash Encoding with resolution level annealing<!-- -->   </span></li><li><span>Optimized camera parameterization and constraint camera multiplex using a projection based loss over all camera proposals for a given view<!-- -->   </span></li><li><span>Per-view importance weighting to leverage the important observation that some views are more useful for optimization than others<!-- -->   </span></li><li><span>Patch-based alignment losses to aid in camera alignment and reconstruction of high-frequency details<!-- -->   </span></li></ul><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-bottom:56.25%;overflow:hidden;height:0"><iframe width="853" height="480" class="absolute top-0 left-0 w-full h-full" src="https://www.youtube.com/embed/V37Q3l6AFAU?si=xrcejZPZxjQ7Uf9K" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameBorder="0"></iframe></div></div></div><div class="space-y-10 pt-10 text-secondary"><div class="space-y-8"></div></div><div class="space-y-3 pt-16"><h1 class="text-primary font-sans text-xl font-bold">Method</h1><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal "><span>In Figure 1 we visualize the SHINOBI optimization pipeline. Two resolution annealed encoding branches, the multiresolution hash grid and the Fourier embedding are used to learn a neural volume conditioned on the input coordinates and illumination. Our patch-based losses and regularization scheme enables robust optimization of camera parameters jointly with the shape, material and per image illumination.<!-- -->   </span></p><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:0"><video autoplay="" loop="" controls="" playsinline="" muted="" class="" style="object-fit:contain;border-radius:0;transform:scale(undefined)"><source src="/videos/shinobi_overview_animated_website_v002.mp4" type="video/mp4" data-src="/videos/shinobi_overview_animated_website_v002.mp4"/></video></div><div class="prose mt-3"><p class="text-justify block font-normal "><strong>Figure 1</strong> — <span>SHINOBI Pipeline Overview<!-- -->   </span></p></div></div></div><div class="space-y-10 pt-10 text-secondary"><div class="space-y-8"></div></div><div class="space-y-3 pt-16"><h1 class="text-primary font-sans text-xl font-bold">Results</h1><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal "><span>The parametric material model allows for controlled editing of the object’s appearance. Also the illumination can be adjusted, e.g. for realistic composites. A mesh extraction allows further editing and integration in the standard graphics pipeline including real-time rendering. SHINOBI can help in obtaining relightable 3D assets for e-commerce applications as well as 3D AR and VR for entertainment and education.<!-- -->   </span></p></div><div class="space-y-10 pt-10 text-secondary"><div class="space-y-8"><div class="prose mt-3"><p class="text-justify block font-normal "><strong>Applications</strong> — <span>We show a scene featuring objects from the <!-- -->   </span><a href="https://navidataset.github.io/" class="text-primary underline cursor-pointer hover:text-secondary">NAVI dataset [1]</a><span> in a new consistent illumination environment as it would be required for AR and VR applications.<!-- -->   </span></p><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:56.25%"><video autoplay="" loop="" playsinline="" muted="" class="absolute top-0 left-0 w-full h-full" style="object-fit:cover;border-radius:0.5rem;transform:scale(1)"><source src="/videos/shinobi_integrated_scene_relighting_v05_compressed.mp4" type="video/mp4" data-src="/videos/shinobi_integrated_scene_relighting_v05_compressed.mp4"/></video></div></div><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:56.25%"><video autoplay="" loop="" playsinline="" muted="" class="absolute top-0 left-0 w-full h-full" style="object-fit:cover;transition:none;opacity:1;z-index:0;border-radius:0.5rem"><source src="/videos/shinobi_material_editing0_v05_compressed.mp4" type="video/mp4"/></video><video autoplay="" loop="" playsinline="" muted="" class="absolute top-0 left-0 w-full h-full" style="object-fit:cover;transition:opacity 500ms ease-in-out;opacity:0;z-index:1;border-radius:0.5rem"><source src="/videos/shinobi_material_editing1_v05_compressed.mp4" type="video/mp4"/></video><div class="absolute bottom-2 right-2 flex flex-row space-x-1.5"><video autoplay="" loop="" playsinline="" muted="" class="w-16 h-16 cursor-pointer shadow-lg border-2" style="object-fit:cover;filter:grayscale(100%);border-color:#fff;z-index:2;border-radius:0.5rem"><source src="/videos/shinobi_material_editing0_v05_compressed.mp4" type="video/mp4"/></video><video autoplay="" loop="" playsinline="" muted="" class="w-16 h-16 cursor-pointer shadow-lg border-2" style="object-fit:cover;filter:none;border-color:rgba(255, 255, 255, 0.5);z-index:2;border-radius:0.5rem"><source src="/videos/shinobi_material_editing1_v05_compressed.mp4" type="video/mp4"/></video></div></div></div></div><div class="prose mt-3"><strong>Comparison to SAMURAI</strong><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:0"><video autoplay="" loop="" playsinline="" muted="" class="" style="object-fit:contain;border-radius:0;transform:scale(undefined)"><source src="/videos/decomposition_comparison_eagle_v001_compressed.mp4" type="video/mp4" data-src="/videos/decomposition_comparison_eagle_v001_compressed.mp4"/></video></div><div class="prose mt-3"><p class="text-justify block font-normal "><strong>Figure 2</strong> — <span>Material decomposition compared to SAMURAI [2] on &#x27;bald eagle&#x27; scene [1].<!-- -->   </span></p></div></div></div><div class="prose mt-3"><p class="text-justify block font-normal "><strong>Reconstructed Assets</strong> — <span>Example results from <!-- -->   </span><a href="https://navidataset.github.io/" class="text-primary underline cursor-pointer hover:text-secondary">NAVI dataset [1]</a><span>. Click on an image for an interactive 3D visualization.<!-- -->   </span></p><p> Error </p><p> Error </p><p> Error </p><p> Error </p><p> Error </p><p> Error </p><p> Error </p><p> Error </p><p> Error </p><p> Error </p></div></div></div><div class="relative w-full bg-slate-200" style="height:400px"><canvas id="renderCanvas" touch-action="none" class="absolute top-0 left-0 w-full h-full"></canvas></div><div class="space-y-3 pt-16"><h1 class="text-primary font-sans text-xl font-bold">Citation</h1><div class="group relative rounded-lg bg-black bg-opacity-5 hover:bg-opacity-10 transition duration-200 ease-in-out p-2"><button class="opacity-0 group-hover:opacity-100 absolute top-2 right-2 p-2 rounded-lg bg-black text-white transition duration-200 ease-in-out cursor-pointer z-10 hover:bg-opacity-80"><img src="/icons/copy.svg" class="w-4 h-4" alt="Copy to Clipboard"/></button><div class="text-justify grid grid-cols-[auto_1fr]"><span>@<!-- -->misc<!-- -->{</span><span>engelhardt2023-shinobi<!-- -->,</span> <span class="pl-12">author =</span><span>{<!-- -->Engelhardt, Andreas and <!-- -->Raj, Amit and <!-- -->Boss, Mark and <!-- -->Zhang, Yunzhi and <!-- -->Kar, Abhishek and <!-- -->Li, Yuanzhen and <!-- -->Sun, Deqing and <!-- -->Barron, Jonathan T. and <!-- -->Lensch, Hendrik P.A. and <!-- -->Jampani, Varun<!-- -->},</span><span class="pl-12">title =</span><span>{<!-- -->{{SHINOBI}: {Sh}ape and {I}llumination using {N}eural {O}bject Decomposition via {B}RDF Optimization {I}n-the-wild}<!-- -->},</span><span class="pl-12">booktitle =</span><span>{<!-- -->{preprint}<!-- -->},</span><span class="pl-12">year =</span><span>{<!-- -->2023<!-- -->}</span><span>}</span></div></div></div><div class="space-y-3 pt-16"><h1 class="text-primary font-sans text-xl font-bold">Acknowledgements</h1><p class="text-justify block font-normal "></p> <p class="text-justify block font-normal "><span>This work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 and SFB 1233, TP 02  -  Project number 276693517. It was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A.<!-- -->   </span></p></div></div></div><script>((STORAGE_KEY, restoreKey) => {
    if (!window.history.state || !window.history.state.key) {
      let key = Math.random().toString(32).slice(2);
      window.history.replaceState({
        key
      }, "");
    }
    try {
      let positions = JSON.parse(sessionStorage.getItem(STORAGE_KEY) || "{}");
      let storedY = positions[restoreKey || window.history.state.key];
      if (typeof storedY === "number") {
        window.scrollTo(0, storedY);
      }
    } catch (error) {
      console.error(error);
      sessionStorage.removeItem(STORAGE_KEY);
    }
  })("positions", null)</script><link rel="modulepreload" href="/build/manifest-CD862A4E.js"/><link rel="modulepreload" href="/build/entry.client-2426CX6J.js"/><link rel="modulepreload" href="/build/_shared/chunk-KRLU3BPM.js"/><link rel="modulepreload" href="/build/_shared/chunk-DWFMXSZ6.js"/><link rel="modulepreload" href="/build/_shared/chunk-YD3KDPF7.js"/><link rel="modulepreload" href="/build/root-C2VGHRKI.js"/><link rel="modulepreload" href="/build/routes/_index-VHZTR7HY.js"/><script>window.__remixContext = {"url":"/","state":{"loaderData":{"root":{"title":{"title":"SHINOBI","subtitle":"Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild"}},"routes/_index":{"title":{"title":"SHINOBI","subtitle":"Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild"},"authors":[{"name":"Andreas Engelhardt","link":"https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/"},{"name":"Amit Raj","link":"https://amitraj93.github.io/"},{"name":"Mark Boss","link":"https://markboss.me/"},{"name":"Yunzhi Zhang","link":"https://cs.stanford.edu/~yzzhang/"},{"name":"Abhishek Kar","link":"https://abhishekkar.info/"},{"name":"Yuanzhen Li","link":"https://people.csail.mit.edu/yzli/"},{"name":"Deqing Sun","link":"https://deqings.github.io/"},{"name":"Jonathan T. Barron","link":"https://jonbarron.info/"},{"name":"Hendrik P.A. Lensch","link":"https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/"},{"name":"Varun Jampani","link":"https://varunjampani.github.io/"}],"institutions":[{"name":"University of Tübingen","link":"https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/","image":"images/uni_logo.png"},{"name":"Google Research","link":"https://research.google/research-areas/machine-perception/","image":"images/google_logo.png"}],"links":[{"icon":"arxiv","name":"Paper","link":"https://arxiv.org"},{"icon":"pdf","name":"Supplementary","link":"arxiv/shinobi_supplementary.pdf"},{"icon":"github","name":"Code (coming soon)","link":""},{"icon":"youtube","name":"Video","link":"https://yotube.com/"},{"icon":"results","name":"Results","link":""},{"icon":"bibtex","name":"Cite","link":""}],"document":{"chapters":[{"name":"Abstract","styling":{"hideHeading":true},"introduction":[{"type":"figure","id":"teaser"},{"type":"text","content":[{"type":"plain_text","content":"We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc."}]}],"sections":[],"paragraphs":[]},{"name":"Overview","introduction":[{"type":"text","content":[{"type":"plain_text","content":"We present a category-agnostic technique to jointly reconstruct 3D shape and material properties of objects from unconstrained in-the-wild image collections. This data regime poses multiple challenges as images are captured in different environments using a variety of devices resulting in varying backgrounds, illumination, camera poses, and intrinsics. Conventional structure-from-motion techniques like COLMAP fail to reconstruct image collections under these challenging conditions. Recent methods like "},{"type":"link_text","content":"SAMURAI [2]","link":"https://markboss.me/publication/2022-samurai/"},{"type":"plain_text","content":" and "},{"type":"link_text","content":"NeRS [3]","link":"https://jasonyzhang.com/ners/"},{"type":"plain_text","content":" can be initialized from very coarse view directions but still yield low quality reconstructions for many challenging scenes. Additionally, optimization takes more than 12 hours in the case of SAMURAI. In contrast, we propose a pipeline based on "},{"type":"link_text","content":"multiresolution hash grids [4]","link":"https://nvlabs.github.io/instant-ngp/"},{"type":"plain_text","content":" which allows us to process more rays in a shorter time. Using this advantage we are able to improve reconstruction quality while still keeping a competitive optimization time (~4 hours). However, naive integration of multi-resolution hash grids is not well suited to camera pose estimation due to discontinuities in the gradients with respect to the input positions. We propose several components that work together to stabilize the camera pose optimization and encourage sharp features:"}]},{"type":"list","content":[{"type":"text","content":[{"type":"plain_text","content":"Hybrid Multiresolution Hash Encoding with resolution level annealing"}]},{"type":"text","content":[{"type":"plain_text","content":"Optimized camera parameterization and constraint camera multiplex using a projection based loss over all camera proposals for a given view"}]},{"type":"text","content":[{"type":"plain_text","content":"Per-view importance weighting to leverage the important observation that some views are more useful for optimization than others"}]},{"type":"text","content":[{"type":"plain_text","content":"Patch-based alignment losses to aid in camera alignment and reconstruction of high-frequency details"}]}]},{"type":"figure","id":"overview"}],"sections":[],"paragraphs":[]},{"name":"Method","introduction":[{"type":"text","content":[{"type":"plain_text","content":"In Figure 1 we visualize the SHINOBI optimization pipeline. Two resolution annealed encoding branches, the multiresolution hash grid and the Fourier embedding are used to learn a neural volume conditioned on the input coordinates and illumination. Our patch-based losses and regularization scheme enables robust optimization of camera parameters jointly with the shape, material and per image illumination."}]},{"type":"figure","id":"pipeline"}],"sections":[],"paragraphs":[]},{"name":"Results","introduction":[{"type":"text","content":[{"type":"plain_text","content":"The parametric material model allows for controlled editing of the object’s appearance. Also the illumination can be adjusted, e.g. for realistic composites. A mesh extraction allows further editing and integration in the standard graphics pipeline including real-time rendering. SHINOBI can help in obtaining relightable 3D assets for e-commerce applications as well as 3D AR and VR for entertainment and education."}]}],"sections":[],"paragraphs":[{"type":"paragraph","name":"Applications","contents":[{"type":"text","content":[{"type":"plain_text","content":"We show a scene featuring objects from the "},{"type":"link_text","content":"NAVI dataset [1]","link":"https://navidataset.github.io/"},{"type":"plain_text","content":" in a new consistent illumination environment as it would be required for AR and VR applications."}]},{"type":"figure","id":"shinobi_scene_relighting"},{"type":"figure","id":"shinobi_scene_material_editing"}]},{"type":"paragraph","name":"Comparison to SAMURAI","contents":[{"type":"figure","id":"decomposition"}]},{"type":"paragraph","name":"Reconstructed Assets","contents":[{"type":"text","content":[{"type":"plain_text","content":"Example results from "},{"type":"link_text","content":"NAVI dataset [1]","link":"https://navidataset.github.io/"},{"type":"plain_text","content":". Click on an image for an interactive 3D visualization."}]},{"type":"3drelight","id":"tractor"},{"type":"3drelight","id":"remotecar"},{"type":"3drelight","id":"keywest"},{"type":"3drelight","id":"lion"},{"type":"3drelight","id":"baldeagle"},{"type":"3drelight","id":"fireengine"},{"type":"3drelight","id":"pumpkin"},{"type":"3drelight","id":"dino_5"},{"type":"3drelight","id":"hutmushroom"},{"type":"3drelight","id":"colored_box"}]}]}]},"figures":{"teaser":{"name":"Teaser","type":"video","urls":["/videos/shinobi_teaser.mp4"],"styling":{"roundedCorners":false,"objectFit":"contain"},"captions":[{"type":"paragraph","name":"SHINOBI","contents":[{"type":"text","content":[{"type":"plain_text","content":"reconstructs shape, illumination and materials from in-the-wild image collections."}]}]}]},"pipeline":{"name":"Pipeline","type":"video","urls":["/videos/shinobi_overview_animated_website_v002.mp4"],"styling":{"roundedCorners":false,"showControls":true,"objectFit":"contain"},"captions":[{"type":"paragraph","name":"Figure 1","contents":[{"type":"text","content":[{"type":"plain_text","content":"SHINOBI Pipeline Overview"}]}]}]},"decomposition":{"name":"Decomposition","type":"video","urls":["/videos/decomposition_comparison_eagle_v001_compressed.mp4"],"styling":{"roundedCorners":false,"showControls":false,"objectFit":"contain"},"captions":[{"type":"paragraph","name":"Figure 2","contents":[{"type":"text","content":[{"type":"plain_text","content":"Material decomposition compared to SAMURAI [2] on 'bald eagle' scene [1]."}]}]}]},"shinobi_scene_relighting":{"name":"Application Relighting","type":"video","urls":["/videos/shinobi_integrated_scene_relighting_v05_compressed.mp4"]},"shinobi_scene_material_editing":{"name":"Application Material Editing","type":"swappable_video","urls":["/videos/shinobi_material_editing0_v05_compressed.mp4","/videos/shinobi_material_editing1_v05_compressed.mp4"]},"overview":{"name":"Overview","type":"youtube","urls":["https://www.youtube.com/embed/V37Q3l6AFAU?si=xrcejZPZxjQ7Uf9K"]}},"bibliography":{"metadata":{"collection":"bibliography","source":"/Users/jdihlmann/Documents/thesis/parser/../bibliography.bib","created":"2023-07-24T07:16:12.427853","records":1},"records":[{"type":"article","id":"SketchGuidance","citekey":"SketchGuidance","collection":"bibliography","title":"Sketch-Guided Text-to-Image Diffusion Models","year":"2022","url":"https://sketch-guided-diffusion.github.io/","ctitle":"SketchGuidance","abstract":"Text-to-Image models have introduced a remarkable leap in the evolution of machine learning, demonstrating high-quality synthesis of images from a given text-prompt. However, these powerful pretrained models still lack control handles that can guide spatial properties of the synthesized images. In this work, we introduce a universal approach to guide a pretrained text-to-image diffusion model, with a spatial map from another domain (e.g., sketch) during inference time. Unlike previous works, our method does not require to train a dedicated model or a specialized encoder for the task. Our key idea is to train a Latent Guidance Predictor (LGP) - a small, per-pixel, Multi-Layer Perceptron (MLP) that maps latent features of noisy images to spatial maps, where the deep features are extracted from the core Denoising Diffusion Probabilistic Model (DDPM) network. The LGP is trained only on a few thousand images and constitutes a differential guiding map predictor, over which the loss is computed and propagated back to push the intermediate images to agree with the spatial map. The per-pixel training offers flexibility and locality which allows the technique to perform well on out-of-domain sketches, including free-hand style drawings. We take a particular focus on the sketch-to-image translation task, revealing a robust and expressive way to generate images that follow the guidance of a sketch of arbitrary style or domain. Project page: sketch-guided-diffusion.github.io","eprint":"2211.13752","file":":http\\://arxiv.org/pdf/2211.13752v1:PDF","journal":"arXiv preprint arXiv:2211.13752","author":[{"name":"Voynov, Andrey"},{"name":"Aberman, Kfir"},{"name":"Cohen-Or, Daniel"}]}]},"citation":{"type":"misc","name":"engelhardt2023-shinobi","authors":[["Andreas","Engelhardt"],["Amit","Raj"],["Mark","Boss"],["Yunzhi","Zhang"],["Abhishek","Kar"],["Yuanzhen","Li"],["Deqing","Sun"],["Jonathan T.","Barron"],["Hendrik P.A.","Lensch"],["Varun","Jampani"]],"title":"{{SHINOBI}: {Sh}ape and {I}llumination using {N}eural {O}bject Decomposition via {B}RDF Optimization {I}n-the-wild}","booktitle":"{preprint}","year":2023,"eprint":"{}","archivePrefix":"{arXiv}","primaryClass":"{cs.CV}"},"acknowledgements":{"name":"Acknowledgements","content":[{"type":"text","content":[{"type":"plain_text","content":"This work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 and SFB 1233, TP 02  -  Project number 276693517. It was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A."}]}]}}},"actionData":null,"errors":null},"future":{"v3_fetcherPersist":false,"v3_relativeSplatPath":false}};</script><script type="module" async="">import "/build/manifest-CD862A4E.js";
import * as route0 from "/build/root-C2VGHRKI.js";
import * as route1 from "/build/routes/_index-VHZTR7HY.js";
window.__remixRouteModules = {"root":route0,"routes/_index":route1};

import("/build/entry.client-2426CX6J.js");</script></body></html>