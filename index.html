<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="description" content="Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild"/><meta property="og:url" content="https://shinobi.aengelhardt.com"/><meta property="og:type" content="website"/><meta property="og:title" content="SHINOBI"/><meta property="og:description" content="Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild"/><meta property="og:image" content="https://shinobi.aengelhardt.com/images/preview.jpg"/><meta name="twitter:card" content="summary_large_image"/><meta property="twitter:domain" content="shinobi.aengelhardt.com"/><meta property="twitter:url" content="https://shinobi.aengelhardt.com"/><meta name="twitter:title" content="SHINOBI"/><meta name="twitter:description" content="Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild"/><meta name="twitter:image" content="https://shinobi.aengelhardt.com/images/preview.jpg"/><link rel="icon" href="favicon.ico"/><link rel="stylesheet" href="/build/_assets/app-NAGIGPFG.css"/><link rel="stylesheet" href="https://rsms.me/inter/inter.css"/><title>SHINOBI</title><script src="babylon/babylon.js"></script><script src="babylon/babylonjs.loaders.min.js"></script><script src="babylon/babylon.gui.min.js"></script><script src="https://code.jquery.com/pep/0.4.3/pep.js"></script></head><body class="bg-white dark:bg-black" style="overscroll-behavior:none"><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H3WNEKDGDG"></script><script async="" id="gtag-init">
								window.dataLayer = window.dataLayer || [];
								function gtag(){dataLayer.push(arguments);}
								gtag('js', new Date());

								gtag('config', 'G-H3WNEKDGDG', {
								page_path: window.location.pathname,
								});
							</script><div class="flex w-full flex-col bg-white dark:bg-black"><div class="mx-auto w-full max-w-4xl px-4 py-12 pb-10 md:px-8"><div class="w-full flex flex-col justify-center items-center space-y-5 pb-10  rounded-lg"><div class="w-full flex flex-col justify-center items-center space-y-5"><h1 class="text-primary font-sans text-3xl font-bold xl:text-5xl text-center dark:text-white">SHINOBI</h1><h2 class="text-primary font-sans text-xl font-medium xl:text-2xl text-center break-words px-2 max-w-[90%] dark:text-white">Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild</h2></div><div class="w-full flex flex-wrap justify-center items-center gap-5"><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Andreas Engelhardt</a><a href="https://amitraj93.github.io/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Amit Raj</a><a href="https://markboss.me/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Mark Boss</a><a href="https://cs.stanford.edu/~yzzhang/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Yunzhi Zhang</a><a href="https://abhishekkar.info/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Abhishek Kar</a><a href="https://people.csail.mit.edu/yzli/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Yuanzhen Li</a><a href="https://deqings.github.io/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Deqing Sun</a><a href="http://ricardomartinbrualla.com/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Ricardo Martin Brualla</a><a href="https://jonbarron.info/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Jonathan T. Barron</a><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Hendrik P.A. Lensch</a><a href="https://varunjampani.github.io/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">Varun Jampani</a></div><div class="w-full flex flex-row justify-center items-center space-x-10"><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/" class="text-secondary font-sans cursor-pointer hover:text-primary  dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out"><img src="images/uni_logo_alpha.png" alt="University of Tübingen" class="w-auto h-20 object-contain"/></a><a href="https://research.google/research-areas/machine-perception/" class="text-secondary font-sans cursor-pointer hover:text-primary  dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out"><img src="images/google_logo_alpha.png" alt="Google Research" class="w-auto h-20 object-contain"/></a></div><div class="w-full flex flex-wrap justify-center items-center gap-5"><button class="flex flex-row items-center space-x-2 rounded-full pl-2 pr-3 py-1 bg-black bg-opacity-70 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer dark:bg-white dark:bg-opacity-10 dark:hover:bg-opacity-20 "><svg width="24" height="24" viewBox="0 0 24 24" class="fill-white w-5 h-5" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_44_12)"><path d="M9.29399 9.49214L11.4664 11.9742L17.8649 4.86716C18.2034 4.4411 18.3634 4.21719 18.2033 3.85459C18.1187 3.66237 17.9782 3.49672 17.7982 3.37674C17.6182 3.25676 17.4061 3.18736 17.1862 3.17653C17.0612 3.16982 16.9362 3.18744 16.8188 3.22829C16.7014 3.26915 16.594 3.33236 16.5035 3.41403L9.29399 9.49214Z"></path><path d="M15.0233 10.6788L4.85512 1.45596C4.65371 1.22019 4.37041 1.05959 4.05557 1.00271C3.84704 0.998134 3.64184 1.05266 3.46637 1.15929C3.2909 1.26591 3.15315 1.41976 3.07093 1.601C2.91082 1.96361 3.0259 2.21833 3.37679 2.69425L11.4684 11.9787L5.44566 18.9733C5.2889 19.1266 5.17773 19.3163 5.12336 19.5232C5.06898 19.7301 5.07334 19.9469 5.136 20.1517C5.21716 20.3382 5.3555 20.4974 5.5332 20.6089C5.7109 20.7204 5.91986 20.779 6.1331 20.7772C6.27085 20.7745 6.40631 20.7435 6.53013 20.6864C6.65395 20.6292 6.76316 20.5473 6.85021 20.4464L15.0175 13.2668C15.2093 13.1037 15.3627 12.9044 15.4679 12.6817C15.5731 12.459 15.6277 12.2179 15.6282 11.974C15.6288 11.7301 15.5753 11.4888 15.4711 11.2657C15.3669 11.0426 15.2143 10.8427 15.0233 10.6788Z" class="fill-opacity-80"></path><path d="M19.6203 21.2731L11.4472 11.9587L9.27579 9.47763L7.94409 10.6144C7.75352 10.7888 7.60199 10.9976 7.4985 11.2285C7.395 11.4594 7.34167 11.7075 7.34167 11.9583C7.34167 12.209 7.395 12.4572 7.4985 12.6881C7.60199 12.919 7.75352 13.1278 7.94409 13.3022L18.3098 22.7471C18.4008 22.8369 18.512 22.9065 18.635 22.9507C18.758 22.9949 18.8899 23.0127 19.0211 23.0027C19.2199 22.9996 19.4131 22.9404 19.5759 22.8326C19.7387 22.7249 19.8637 22.5735 19.9348 22.3981C19.9937 22.2011 19.9956 21.9928 19.9403 21.7949C19.885 21.597 19.7745 21.4168 19.6203 21.2731Z"></path></g><defs><clipPath id="clip0_44_12"><rect width="17" height="22" transform="translate(3 1)"></rect></clipPath></defs></svg><span class="text-m  text-white">Paper</span></button><button class="flex flex-row items-center space-x-2 rounded-full pl-2 pr-3 py-1 bg-black bg-opacity-70 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer dark:bg-white dark:bg-opacity-10 dark:hover:bg-opacity-20 "><svg width="24" height="24" viewBox="0 0 24 24" class="fill-white w-5 h-5" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_44_10)"><path d="M15.5221 23.9239C20.4308 22.3902 24 17.7347 24 12.2295C24 5.47535 18.6274 0 12 0C5.37258 0 0 5.47535 0 12.2295C0 17.825 3.68728 22.5428 8.72061 23.9968C9.16223 24.0298 9.32663 23.7163 9.32663 23.4441C9.32663 23.3219 9.32468 23.106 9.32206 22.8172C9.31903 22.4826 9.31512 22.05 9.31232 21.5519C6.3659 22.2241 5.74423 20.0601 5.74423 20.0601C5.26237 18.7752 4.56787 18.433 4.56787 18.433C3.60611 17.7424 4.64071 17.7561 4.64071 17.7561C5.70392 17.8353 6.26316 18.903 6.26316 18.903C7.20801 20.6031 8.74268 20.112 9.34614 19.8279C9.44238 19.1086 9.7155 18.6181 10.0185 18.3401C7.66646 18.0594 5.19344 17.1044 5.19344 12.8407C5.19344 11.6261 5.60637 10.6329 6.28397 9.85492C6.17472 9.57349 5.81121 8.44231 6.38736 6.91017C6.38736 6.91017 7.27694 6.61098 9.30062 8.05091C10.1453 7.80432 11.0518 7.68068 11.9525 7.67658C12.8518 7.68068 13.7583 7.80432 14.6043 8.05091C16.6267 6.61098 17.5143 6.91017 17.5143 6.91017C18.0924 8.44231 17.7289 9.57349 17.6197 9.85492C18.2986 10.6329 18.7089 11.6261 18.7089 12.8407C18.7089 17.1154 16.232 18.056 13.8721 18.3312C14.2525 18.6748 14.5913 19.3538 14.5913 20.3921C14.5913 21.3866 14.5855 22.2528 14.5816 22.8266C14.5797 23.1102 14.5783 23.3234 14.5783 23.4441C14.5783 23.7419 14.7688 24.0882 15.3066 23.9796C15.3755 23.9554 15.4474 23.937 15.5221 23.9239Z"></path></g><defs><clipPath id="clip0_44_10"><rect width="24" height="24"></rect></clipPath></defs></svg><span class="text-m  text-white">Code (coming soon)</span></button><button class="flex flex-row items-center space-x-2 rounded-full pl-2 pr-3 py-1 bg-black bg-opacity-70 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer dark:bg-white dark:bg-opacity-10 dark:hover:bg-opacity-20 "><svg width="24" height="24" viewBox="0 0 24 24" class="fill-white w-5 h-5" xmlns="http://www.w3.org/2000/svg"><path d="M12.0136 20H11.9916C11.9235 20 5.10583 19.9872 3.38575 19.5322C2.43989 19.284 1.70122 18.5641 1.44695 17.6427C1.13764 15.9432 0.988277 14.2196 1.00072 12.4936C0.992726 10.7647 1.14613 9.03873 1.45904 7.33693C1.7203 6.41469 2.45573 5.6915 3.40004 5.4282C5.07285 5 11.7026 5 11.9839 5H12.007C12.0763 5 18.9115 5.01285 20.614 5.46781C21.5579 5.71725 22.2951 6.43566 22.5506 7.35512C22.8698 9.06103 23.0196 10.7929 22.998 12.5268C23.0057 14.2535 22.8519 15.9774 22.5385 17.677C22.281 18.5972 21.5418 19.3155 20.5964 19.5643C18.9258 19.9957 12.295 20 12.0136 20ZM9.81544 9.2874L9.80995 15.7105L15.5384 12.4989L9.81544 9.2874Z"></path></svg><span class="text-m  text-white">Video</span></button><button class="flex flex-row items-center space-x-2 rounded-full pl-2 pr-3 py-1 bg-black bg-opacity-70 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer dark:bg-white dark:bg-opacity-10 dark:hover:bg-opacity-20 "><svg width="24" height="24" viewBox="0 0 24 24" class="fill-white w-5 h-5" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_102_33)"><path d="M19.7898 21.8748L13.1223 15.2062C10.1563 17.3149 6.06918 16.7991 3.72002 14.0196C1.37086 11.2401 1.54335 7.12419 4.11684 4.551C6.68964 1.97669 10.8059 1.80345 13.586 4.15249C16.366 6.50152 16.8821 10.589 14.7732 13.5553L21.4407 20.224L19.791 21.8737L19.7898 21.8748ZM9.06584 4.83332C6.85349 4.83282 4.94481 6.38579 4.49539 8.55201C4.04597 10.7182 5.17933 12.9023 7.20929 13.7819C9.23925 14.6616 11.6079 13.995 12.8811 12.1857C14.1544 10.3765 13.9822 7.92187 12.469 6.308L13.1748 7.008L12.3792 6.21467L12.3652 6.20067C11.4923 5.32237 10.3041 4.82998 9.06584 4.83332Z"></path></g><defs><clipPath id="clip0_102_33"><rect width="24" height="24"></rect></clipPath></defs></svg><span class="text-m  text-white">Results</span></button><button class="flex flex-row items-center space-x-2 rounded-full pl-2 pr-3 py-1 bg-black bg-opacity-70 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer dark:bg-white dark:bg-opacity-10 dark:hover:bg-opacity-20 "><svg width="24" height="24" viewBox="0 0 24 24" class="fill-white w-5 h-5" xmlns="http://www.w3.org/2000/svg"><path d="M9.29631 4H9.02868C6.92169 5.19793 5.31595 6.36613 4.21147 7.50458C2.07049 9.67955 1 12.0967 1 14.7559C1 16.5145 1.46728 17.9121 2.40183 18.9486C3.37037 20.0191 4.59804 20.5544 6.08483 20.5544C7.29126 20.5544 8.27254 20.1593 9.02868 19.3692C9.78482 18.6046 10.1629 17.585 10.1629 16.3106C10.1629 15.1382 9.78482 14.2079 9.02868 13.5197C8.34051 12.8231 7.49516 12.4747 6.49264 12.4747C6.32272 12.4747 6.14855 12.4917 5.97014 12.5257L5.4094 12.5767C5.06107 12.5767 4.80194 12.4917 4.63202 12.3218L4.37714 11.4934C4.37714 10.287 5.04833 9.01262 6.39069 7.67025C7.14683 6.90562 8.28529 6.00505 9.80606 4.96854L9.29631 4ZM21.7216 4H21.454C19.347 5.19793 17.7413 6.36613 16.6368 7.50458C14.4958 9.67955 13.4253 12.0967 13.4253 14.7559C13.4253 16.5145 13.8926 17.9121 14.8272 18.9486C15.7957 20.0191 17.0191 20.5544 18.4974 20.5544C19.7124 20.5544 20.6979 20.1593 21.454 19.3692C22.2102 18.6046 22.5882 17.585 22.5882 16.3106C22.5882 15.1382 22.2102 14.2079 21.454 13.5197C20.7659 12.8231 19.9205 12.4747 18.918 12.4747C18.7481 12.4747 18.5739 12.4917 18.3955 12.5257L17.8347 12.5767C17.4864 12.5767 17.2273 12.4917 17.0574 12.3218L16.7897 11.4934C16.7897 10.287 17.4652 9.01262 18.816 7.67025C19.5722 6.90562 20.7106 6.00505 22.2314 4.96854L21.7216 4Z"></path></svg><span class="text-m  text-white">Cite</span></button></div></div><div class="pt-2"><div class="pt-4" style="filter:none"><div class="w-full h-full overflow-hidden" style="border-radius:0.5rem"><video autoplay="" loop="" playsinline="" muted="" class="w-full h-full" style="object-fit:contain;transform:scale(1);transform-origin:center center"><source src="videos/shinobi_teaser.mp4" type="video/mp4"/></video></div><div class="prose mt-3 pb-6"><p class="text-justify block font-normal text-primary dark:text-white"><strong>SHINOBI</strong> — <span>reconstructs shape, illumination and materials from in-the-wild image collections.<!-- --> </span></p></div></div></div><div class="space-y-3 pt-16" id="Abstract"><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal text-primary dark:text-white"><span>We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object&#x27;s shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc.<!-- --> </span></p></div><div class="space-y-10 pt-10 text-secondary dark:text-white"><div class="space-y-8"></div></div><div class="space-y-3 pt-16" id="Overview"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Overview</h1><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal text-primary dark:text-white"><span>SHINOBI is a category-agnostic technique to jointly reconstruct 3D shape and material properties of objects from unconstrained in-the-wild image collections. This data regime poses multiple challenges as images are captured in different environments using a variety of devices resulting in varying backgrounds, illumination, camera poses, and intrinsics. Conventional structure-from-motion techniques like COLMAP fail to reconstruct image collections under these challenging conditions. Recent methods like <!-- --> </span><a href="https://markboss.me/publication/2022-samurai/" class="group text-primary font-sans underline cursor-pointer text-inherit text-center text-opacity-70 hover:text-opacity-100 dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">SAMURAI [2]</a><span> and <!-- --> </span><a href="https://jasonyzhang.com/ners/" class="group text-primary font-sans underline cursor-pointer text-inherit text-center text-opacity-70 hover:text-opacity-100 dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">NeRS [3]</a><span> can be initialized from very coarse view directions but still yield low quality reconstructions for many challenging scenes. Additionally, optimization takes more than 12 hours in the case of SAMURAI. In contrast, we propose a pipeline based on <!-- --> </span><a href="https://nvlabs.github.io/instant-ngp/" class="group text-primary font-sans underline cursor-pointer text-inherit text-center text-opacity-70 hover:text-opacity-100 dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">multiresolution hash grids [4]</a><span> which allows us to process more rays in a shorter time. Using this advantage we are able to improve reconstruction quality while still keeping a competitive optimization time (~4 hours). However, naive integration of multi-resolution hash grids is not well suited to camera pose estimation due to discontinuities in the gradients with respect to the input positions. We propose several components that work together to stabilize the camera pose optimization and encourage sharp features:<!-- --> </span></p><ul class="list-disc list-outside ml-5 text-justify block font-normal text-primary dark:text-white"><li><span>Hybrid Multiresolution Hash Encoding with resolution level annealing<!-- --> </span></li><li><span>Optimized camera parameterization and constraint camera multiplex using a projection based loss over all camera proposals for a given view<!-- --> </span></li><li><span>Per-view importance weighting to leverage the important observation that some views are more useful for optimization than others<!-- --> </span></li><li><span>Patch-based alignment losses to aid in camera alignment and reconstruction of high-frequency details<!-- --> </span></li></ul><div class="text-primary dark:text-white"><div class="pt-4" style="filter:none"><div class="relative w-full" style="padding-bottom:56.25%;overflow:hidden;height:0"><iframe width="853" height="480" class="absolute top-0 left-0 w-full h-full" src="https://www.youtube.com/embed/iFENQ6AcYd8?si=UCh7iy3EhW3St1OX" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameBorder="0"></iframe></div></div></div></div><div class="space-y-10 pt-10 text-secondary dark:text-white"><div class="space-y-8"></div></div><div class="space-y-3 pt-16" id="Method"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Method</h1><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal text-primary dark:text-white"><span>In the figure below we visualize the SHINOBI optimization pipeline. Two resolution annealed encoding branches, the multiresolution hash grid and the Fourier embedding are used to learn a neural volume conditioned on the input coordinates and illumination. Our patch-based losses and regularization scheme enables robust optimization of camera parameters jointly with the shape, material and per image illumination.<!-- --> </span></p><div class="text-primary dark:text-white"><div class="pt-4" style="filter:none"><div class="w-full h-full overflow-hidden" style="border-radius:0.5rem"><video autoplay="" loop="" controls="" playsinline="" muted="" class="w-full h-full" style="object-fit:contain;transform:scale(1);transform-origin:center center"><source src="videos/shinobi_overview_animated_website_v002.mp4" type="video/mp4"/></video></div></div></div></div><div class="space-y-10 pt-10 text-secondary dark:text-white"><div class="space-y-8"></div></div><div class="space-y-3 pt-16" id="Results"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Results</h1><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal text-primary dark:text-white"><span>The parametric material model allows for controlled editing of the object’s appearance. Also the illumination can be adjusted, e.g. for realistic composites. A mesh extraction allows further editing and integration in the standard graphics pipeline including real-time rendering. SHINOBI can help in obtaining relightable 3D assets for e-commerce applications as well as 3D AR and VR for entertainment and education.<!-- --> </span></p></div><div class="space-y-10 pt-10 text-secondary dark:text-white"><div class="space-y-8"><div class="prose mt-3 pb-6"><p class="text-justify block font-normal text-primary dark:text-white"><strong>Applications</strong> — <span>We show a scene featuring objects from the <!-- --> </span><a href="https://navidataset.github.io/" class="group text-primary font-sans underline cursor-pointer text-inherit text-center text-opacity-70 hover:text-opacity-100 dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">NAVI dataset [1]</a><span> in a new consistent illumination environment as it would be required for AR and VR applications.<!-- --> </span></p><div class="text-primary dark:text-white"><div class="pt-4" style="filter:none"><div class="w-full h-full overflow-hidden" style="border-radius:0.5rem"><video autoplay="" loop="" playsinline="" muted="" class="w-full h-full" style="object-fit:cover;transform:scale(1);transform-origin:center center"><source src="videos/shinobi_integrated_scene_relighting_v05_compressed.mp4" type="video/mp4"/></video></div></div></div><div class="text-primary dark:text-white"><div class="pt-4" style="filter:none"><div class="relative w-full" style="padding-top:56.25%"><video autoplay="" loop="" playsinline="" muted="" class="absolute top-0 left-0 w-full h-full" style="object-fit:cover;transition:none;opacity:1;z-index:0;border-radius:0.5rem"><source src="videos/shinobi_material_editing0_v05_compressed.mp4" type="video/mp4"/></video><video autoplay="" loop="" playsinline="" muted="" class="absolute top-0 left-0 w-full h-full" style="object-fit:cover;transition:opacity 500ms ease-in-out;opacity:0;z-index:1;border-radius:0.5rem"><source src="videos/shinobi_material_editing1_v05_compressed.mp4" type="video/mp4"/></video><div class="absolute bottom-2 right-2 flex flex-row space-x-1.5"><video autoplay="" loop="" playsinline="" muted="" class="cursor-pointer shadow-lg border-2   md:w-16 md:h-16  w-12 h-12" style="object-fit:cover;filter:grayscale(100%);border-color:#fff;z-index:2;border-radius:0.5rem"><source src="videos/shinobi_material_editing0_v05_compressed.mp4" type="video/mp4"/></video><video autoplay="" loop="" playsinline="" muted="" class="cursor-pointer shadow-lg border-2   md:w-16 md:h-16  w-12 h-12" style="object-fit:cover;filter:none;border-color:rgba(255, 255, 255, 0.5);z-index:2;border-radius:0.5rem"><source src="videos/shinobi_material_editing1_v05_compressed.mp4" type="video/mp4"/></video></div></div></div></div></div><div class="prose mt-3 pb-6"><p class="text-justify block font-normal text-primary dark:text-white"><strong>Comparison to SAMURAI</strong> — <span>We reconstruct PBR material parameters basecolor, metallic and roughness. Compared to <!-- --> </span><a href="https://markboss.me/publication/2022-samurai/" class="group text-primary font-sans underline cursor-pointer text-inherit text-center text-opacity-70 hover:text-opacity-100 dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">SAMURAI [2]</a><span> high-frequency details are better preserved while optimization time is reduced to roughly a third. Here we show the &quot;bald eagle&quot; object from the<!-- --> </span><a href="https://navidataset.github.io/" class="group text-primary font-sans underline cursor-pointer text-inherit text-center text-opacity-70 hover:text-opacity-100 dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">NAVI dataset [1]</a><span>.<!-- --> </span></p><div class="text-primary dark:text-white"><div class="pt-4" style="filter:none"><div class="w-full h-full overflow-hidden" style="border-radius:0.5rem"><video autoplay="" loop="" playsinline="" muted="" class="w-full h-full" style="object-fit:contain;transform:scale(1);transform-origin:center center"><source src="videos/decomposition_comparison_eagle_v001_compressed.mp4" type="video/mp4"/></video></div></div></div></div><div class="prose mt-3 pb-6"><p class="text-justify block font-normal text-primary dark:text-white"><strong>Reconstructed Assets</strong> — <span>Example results from <!-- --> </span><a href="https://navidataset.github.io/" class="group text-primary font-sans underline cursor-pointer text-inherit text-center text-opacity-70 hover:text-opacity-100 dark:text-white dark:text-opacity-80 dark:hover:text-white dark:hover:text-opacity-100 transition duration-200 ease-in-out">NAVI dataset [1]</a><span>. Click on an image for an interactive 3D visualization. Select different environment maps for illumination below.<!-- --> </span></p></div></div></div><div class="flex flex-col justify-center items-center gap-2"><div class="w-full h-20 grid grid-cols-fill gap-2 grid-flow-col "><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/tractor.jpg" alt="tractor" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/remotecar.jpg" alt="remotecar" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/keywest.jpg" alt="keywest" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/lion.jpg" alt="lion" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/baldeagle.jpg" alt="baldeagle" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/fireengine.jpg" alt="fireengine" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/pumpkin.jpg" alt="pumpkin" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/dino_5.jpg" alt="dino_5" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/hutmushroom.jpg" alt="hutmushroom" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/colored_box.jpg" alt="colored_box" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button></div><div class="relative w-full bg-slate-200 rounded-lg" style="height:30rem;overflow:hidden"><div class="absolute top-0 left-0 w-full h-full " style="filter:blur(100px)"><img class="w-full h-full object-cover rounded-lg" src="/hdris/lebombo.jpg" alt="lebombo"/></div><div style="position:relative;width:100%;height:100%;overflow:hidden;pointer-events:auto" class="absolute top-0 left-0 w-full h-full"><div style="width:100%;height:100%"><canvas style="display:block"></canvas></div></div></div><div class="w-full h-20 grid grid-cols-fill gap-2 grid-flow-col "><button class="relative w-auto h-auto hover:opacity-80"><img src="/hdris/lebombo.jpg" alt="lebombo" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:2 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/hdris/photo_studio.jpg" alt="photo_studio" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:2 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/hdris/forest_slope.jpg" alt="forest_slope" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:2 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/hdris/urban_alley.jpg" alt="urban_alley" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:2 / 1"/></button></div></div><div class="space-y-3 pt-16" id="Citation"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Citation</h1><div class="group relative rounded-lg bg-black bg-opacity-5 hover:bg-opacity-10 p-2 dark:bg-white dark:bg-opacity-5 dark:hover:bg-opacity-10 transition duration-200 ease-in-out"><button class="opacity-0 group-hover:opacity-100 absolute top-2 right-2 p-2 rounded-lg bg-black text-white cursor-pointer z-10 bg-opacity-50 hover:bg-opacity-70 dark:bg-white dark:bg-opacity-10 dark:hover:bg-opacity-20 dark:text-black transition duration-200 ease-in-out"><svg width="24" height="24" viewBox="0 0 24 24" class="fill-white w-4 h-4" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_51_28)"><path d="M14.3333 23.6667H2.66667C2.04142 23.6884 1.43522 23.4495 0.992837 23.0072C0.550454 22.5648 0.311606 21.9586 0.333333 21.3333V9.66667C0.311606 9.04142 0.550454 8.43522 0.992837 7.99284C1.43522 7.55045 2.04142 7.31161 2.66667 7.33333H7.33333V2.66667C7.31161 2.04142 7.55045 1.43522 7.99284 0.992837C8.43522 0.550454 9.04142 0.311606 9.66667 0.333333H21.3333C21.9586 0.311606 22.5648 0.550454 23.0072 0.992837C23.4495 1.43522 23.6884 2.04142 23.6667 2.66667V14.3333C23.688 14.9585 23.4491 15.5645 23.0068 16.0068C22.5645 16.4491 21.9585 16.688 21.3333 16.6667H16.6667V21.3333C16.688 21.9585 16.4491 22.5645 16.0068 23.0068C15.5645 23.4491 14.9585 23.688 14.3333 23.6667ZM2.66667 9.66667V21.3333H14.3333V16.6667H9.66667C9.04152 16.688 8.43552 16.4491 7.99322 16.0068C7.55092 15.5645 7.31196 14.9585 7.33333 14.3333V9.66667H2.66667ZM9.66667 2.66667V14.3333H21.3333V2.66667H9.66667Z"></path></g><defs><clipPath id="clip0_51_28"><rect width="24" height="24"></rect></clipPath></defs></svg></button><div class="text-justify grid grid-cols-[auto_1fr] text-primary dark:text-white"><span>@<!-- -->misc<!-- -->{</span><span>engelhardt2023-shinobi<!-- -->,</span> <span class="pl-12">author =</span><span>{<!-- -->Engelhardt, Andreas and <!-- -->Raj, Amit and <!-- -->Boss, Mark and <!-- -->Zhang, Yunzhi and <!-- -->Kar, Abhishek and <!-- -->Li, Yuanzhen and <!-- -->Sun, Deqing and <!-- -->Barron, Jonathan T. and <!-- -->Lensch, Hendrik P.A. and <!-- -->Jampani, Varun<!-- -->},</span><span class="pl-12">title =</span><span>{<!-- -->{SHINOBI}: {Sh}ape and {I}llumination using {N}eural {O}bject Decomposition via {B}RDF Optimization {I}n-the-wild<!-- -->},</span><span class="pl-12">booktitle =</span><span>{<!-- -->preprint<!-- -->},</span><span class="pl-12">year =</span><span>{<!-- -->2023<!-- -->}</span><span>}</span></div></div></div><div class="space-y-3 pt-16" id="Acknowledgements"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Acknowledgements</h1><p class="text-primary text-justify block font-normal dark:text-white"></p> <p class="text-justify block font-normal text-primary dark:text-white"><span>This work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 and SFB 1233, TP 02  -  Project number 276693517. It was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A.<!-- --> </span></p></div><div class="space-y-3 pt-16"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">References</h1><p class="text-justify block font-normal "></p> <p class="text-primary text-justify block font-normal dark:text-white"> <!-- -->[1]<!-- --> <!-- -->V. Jampani et al., NAVI: Category-agnostic image collections with high-quality 3D shape and pose annotations, in NeurIPS, 2023.<!-- --> </p><p class="text-primary text-justify block font-normal dark:text-white"> <!-- -->[2]<!-- --> <!-- -->M. Boss et al., SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections, in NeurIPS, 2022.<!-- --> </p><p class="text-primary text-justify block font-normal dark:text-white"> <!-- -->[3]<!-- --> <!-- -->J. Zhang, G. Yang, S. Tulsiani, and D. Ramanan, NeRS: Neural reflectance surfaces for sparse-view 3D reconstruction in the wild, in NeurIPS, 2021.<!-- --> </p><p class="text-primary text-justify block font-normal dark:text-white"> <!-- -->[4]<!-- --> <!-- -->T. Müller, A. Evans, C. Schied, and A. Keller, Instant neural graphics primitives with a multiresolution hash encoding, ACM Trans. Graph., 2022.<!-- --> </p></div></div></div><div class="absolute w-full"><div class=" mx-auto w-full max-w-4xl px-4  md:px-8 flex flex-col justify-between  pb-2 py-2"><div class="w-full h-0.5 bg-gray-200 dark:bg-gray-800"></div><div class=" w-full py-4 flex flex-row justify-between"><div class="flex flex-row  justify-center items-center space-x-3"><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/" class="cursor-pointer hover:underline opacity-70 hover:opacity-100 transition-opacity duration-200 ease-in-out"><p class="text-primary text-justify block  font-medium  text-md dark:text-white ">CG Tübingen</p></a></div><div class="flex flex-row space-x-3 justify-center items-center"><a href="https://uni-tuebingen.de/impressum/" class="cursor-pointer hover:underline opacity-70 hover:opacity-100 transition-opacity duration-200 ease-in-out"><p class="text-primary text-justify block text-sm  dark:text-white ">Impressum</p></a><a href="https://www.uni-tuebingen.de/datenschutzerklaerung/" class="cursor-pointer hover:underline opacity-70 hover:opacity-100 transition-opacity duration-200 ease-in-out"><p class="text-primary text-justify block  text-sm dark:text-white ">Datenschutz</p></a><p> </p><a href="https://twitter.com/CG_Tuebingen" class="text-secondary font-normal  cursor-pointer text-inherit  text-center hover:underline dark:text-white"><svg width="24" height="24" viewBox="0 0 24 24" class="fill-black dark:fill-white w-5 h-5 inline-block opacity-70 hover:opacity-100 transition-opacity duration-200 ease-in-out" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_127_17)"><path d="M21.0552 5.15358C22.1265 4.52278 22.9282 3.52951 23.3104 2.35924C22.3037 2.94754 21.2022 3.36195 20.0536 3.58456C18.4612 1.92552 15.9382 1.52179 13.8943 2.59895C11.8505 3.67612 10.7919 5.9675 11.31 8.19275C7.18626 7.98885 3.3443 6.07041 0.740128 2.91481C-0.618941 5.22352 0.0755721 8.17484 2.32728 9.65936C1.51304 9.63358 0.716855 9.41646 0.0051152 9.02611C0.0051152 9.04729 0.0051152 9.06848 0.0051152 9.08967C0.00558619 11.4946 1.72657 13.5661 4.11999 14.0427C3.36474 14.2451 2.57253 14.2749 1.80381 14.1298C2.47691 16.1865 4.40151 17.5954 6.59513 17.6375C4.77831 19.0419 2.53461 19.8035 0.225022 19.7997C-0.18435 19.8003 -0.5934 19.7771 -1 19.7303C1.34534 21.2146 4.07502 22.0024 6.86285 21.9997C10.7414 22.0259 14.4688 20.52 17.2113 17.8188C19.9538 15.1175 21.4825 11.4464 21.4555 7.62658C21.4555 7.40765 21.4504 7.18989 21.44 6.97331C22.4445 6.25837 23.3114 5.37269 24 4.35789C23.0642 4.7664 22.0716 5.03461 21.0552 5.15358Z"></path></g><defs><clipPath id="clip0_127_17"><rect width="24" height="24"></rect></clipPath></defs></svg></a><a href="https://github.com/cgtuebingen" class="text-secondary font-normal  cursor-pointer text-inherit  text-center hover:underline dark:text-white"><svg width="24" height="24" viewBox="0 0 24 24" class="fill-black dark:fill-white w-5 h-5 inline-block opacity-70 hover:opacity-100 transition-opacity duration-200 ease-in-out" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_44_10)"><path d="M15.5221 23.9239C20.4308 22.3902 24 17.7347 24 12.2295C24 5.47535 18.6274 0 12 0C5.37258 0 0 5.47535 0 12.2295C0 17.825 3.68728 22.5428 8.72061 23.9968C9.16223 24.0298 9.32663 23.7163 9.32663 23.4441C9.32663 23.3219 9.32468 23.106 9.32206 22.8172C9.31903 22.4826 9.31512 22.05 9.31232 21.5519C6.3659 22.2241 5.74423 20.0601 5.74423 20.0601C5.26237 18.7752 4.56787 18.433 4.56787 18.433C3.60611 17.7424 4.64071 17.7561 4.64071 17.7561C5.70392 17.8353 6.26316 18.903 6.26316 18.903C7.20801 20.6031 8.74268 20.112 9.34614 19.8279C9.44238 19.1086 9.7155 18.6181 10.0185 18.3401C7.66646 18.0594 5.19344 17.1044 5.19344 12.8407C5.19344 11.6261 5.60637 10.6329 6.28397 9.85492C6.17472 9.57349 5.81121 8.44231 6.38736 6.91017C6.38736 6.91017 7.27694 6.61098 9.30062 8.05091C10.1453 7.80432 11.0518 7.68068 11.9525 7.67658C12.8518 7.68068 13.7583 7.80432 14.6043 8.05091C16.6267 6.61098 17.5143 6.91017 17.5143 6.91017C18.0924 8.44231 17.7289 9.57349 17.6197 9.85492C18.2986 10.6329 18.7089 11.6261 18.7089 12.8407C18.7089 17.1154 16.232 18.056 13.8721 18.3312C14.2525 18.6748 14.5913 19.3538 14.5913 20.3921C14.5913 21.3866 14.5855 22.2528 14.5816 22.8266C14.5797 23.1102 14.5783 23.3234 14.5783 23.4441C14.5783 23.7419 14.7688 24.0882 15.3066 23.9796C15.3755 23.9554 15.4474 23.937 15.5221 23.9239Z"></path></g><defs><clipPath id="clip0_44_10"><rect width="24" height="24"></rect></clipPath></defs></svg></a></div></div></div></div><script>((STORAGE_KEY, restoreKey) => {
    if (!window.history.state || !window.history.state.key) {
      let key = Math.random().toString(32).slice(2);
      window.history.replaceState({
        key
      }, "");
    }
    try {
      let positions = JSON.parse(sessionStorage.getItem(STORAGE_KEY) || "{}");
      let storedY = positions[restoreKey || window.history.state.key];
      if (typeof storedY === "number") {
        window.scrollTo(0, storedY);
      }
    } catch (error) {
      console.error(error);
      sessionStorage.removeItem(STORAGE_KEY);
    }
  })("positions", null)</script><link rel="modulepreload" href="/build/manifest-B5F25652.js"/><link rel="modulepreload" href="/build/entry.client-J2DQUVRC.js"/><link rel="modulepreload" href="/build/_shared/chunk-D7TEDUQN.js"/><link rel="modulepreload" href="/build/_shared/chunk-3Z32TANI.js"/><link rel="modulepreload" href="/build/_shared/chunk-T36URGAI.js"/><link rel="modulepreload" href="/build/_shared/chunk-6NNCTYAU.js"/><link rel="modulepreload" href="/build/root-WX5QICCQ.js"/><link rel="modulepreload" href="/build/_shared/chunk-GYZ4EGJZ.js"/><link rel="modulepreload" href="/build/_shared/chunk-S4FTTNEK.js"/><link rel="modulepreload" href="/build/routes/_index-DE6JVP6Q.js"/><script>window.__remixContext = {"url":"/","state":{"loaderData":{"root":{"title":{"title":"SHINOBI","subtitle":"Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild"}},"routes/_index":{"title":{"title":"SHINOBI","subtitle":"Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild"},"authors":[{"name":"Andreas Engelhardt","link":"https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/"},{"name":"Amit Raj","link":"https://amitraj93.github.io/"},{"name":"Mark Boss","link":"https://markboss.me/"},{"name":"Yunzhi Zhang","link":"https://cs.stanford.edu/~yzzhang/"},{"name":"Abhishek Kar","link":"https://abhishekkar.info/"},{"name":"Yuanzhen Li","link":"https://people.csail.mit.edu/yzli/"},{"name":"Deqing Sun","link":"https://deqings.github.io/"},{"name":"Ricardo Martin Brualla","link":"http://ricardomartinbrualla.com/"},{"name":"Jonathan T. Barron","link":"https://jonbarron.info/"},{"name":"Hendrik P.A. Lensch","link":"https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/"},{"name":"Varun Jampani","link":"https://varunjampani.github.io/"}],"institutions":[{"name":"University of Tübingen","link":"https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/","image":"images/uni_logo_alpha.png"},{"name":"Google Research","link":"https://research.google/research-areas/machine-perception/","image":"images/google_logo_alpha.png"}],"links":[{"icon":"arxiv","name":"Paper","link":"hhttps://arxiv.org/abs/2401.10171","styling":{"includeInHeader":true,"includeInStickyHeader":true}},{"icon":"github","name":"Code (coming soon)","link":"","styling":{"includeInHeader":true,"includeInStickyHeader":false}},{"icon":"youtube","name":"Video","link":"https://youtu.be/iFENQ6AcYd8","styling":{"includeInHeader":true,"includeInStickyHeader":false}},{"icon":"results","name":"Results","link":"#Results","styling":{"includeInHeader":true,"includeInStickyHeader":true}},{"icon":"bibtex","name":"Cite","link":"#Citation","styling":{"includeInHeader":true,"includeInStickyHeader":true}}],"document":{"chapters":[{"name":"Abstract","styling":{"hideHeading":true},"introduction":[{"type":"text","content":[{"type":"plain_text","content":"We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc."}]}],"sections":[],"paragraphs":[]},{"name":"Overview","introduction":[{"type":"text","content":[{"type":"plain_text","content":"SHINOBI is a category-agnostic technique to jointly reconstruct 3D shape and material properties of objects from unconstrained in-the-wild image collections. This data regime poses multiple challenges as images are captured in different environments using a variety of devices resulting in varying backgrounds, illumination, camera poses, and intrinsics. Conventional structure-from-motion techniques like COLMAP fail to reconstruct image collections under these challenging conditions. Recent methods like "},{"type":"link_text","content":"SAMURAI [2]","link":"https://markboss.me/publication/2022-samurai/"},{"type":"plain_text","content":" and "},{"type":"link_text","content":"NeRS [3]","link":"https://jasonyzhang.com/ners/"},{"type":"plain_text","content":" can be initialized from very coarse view directions but still yield low quality reconstructions for many challenging scenes. Additionally, optimization takes more than 12 hours in the case of SAMURAI. In contrast, we propose a pipeline based on "},{"type":"link_text","content":"multiresolution hash grids [4]","link":"https://nvlabs.github.io/instant-ngp/"},{"type":"plain_text","content":" which allows us to process more rays in a shorter time. Using this advantage we are able to improve reconstruction quality while still keeping a competitive optimization time (~4 hours). However, naive integration of multi-resolution hash grids is not well suited to camera pose estimation due to discontinuities in the gradients with respect to the input positions. We propose several components that work together to stabilize the camera pose optimization and encourage sharp features:"}]},{"type":"list","content":[{"type":"text","content":[{"type":"plain_text","content":"Hybrid Multiresolution Hash Encoding with resolution level annealing"}]},{"type":"text","content":[{"type":"plain_text","content":"Optimized camera parameterization and constraint camera multiplex using a projection based loss over all camera proposals for a given view"}]},{"type":"text","content":[{"type":"plain_text","content":"Per-view importance weighting to leverage the important observation that some views are more useful for optimization than others"}]},{"type":"text","content":[{"type":"plain_text","content":"Patch-based alignment losses to aid in camera alignment and reconstruction of high-frequency details"}]}]},{"type":"figure","id":"overview"}],"sections":[],"paragraphs":[]},{"name":"Method","introduction":[{"type":"text","content":[{"type":"plain_text","content":"In the figure below we visualize the SHINOBI optimization pipeline. Two resolution annealed encoding branches, the multiresolution hash grid and the Fourier embedding are used to learn a neural volume conditioned on the input coordinates and illumination. Our patch-based losses and regularization scheme enables robust optimization of camera parameters jointly with the shape, material and per image illumination."}]},{"type":"figure","id":"pipeline"}],"sections":[],"paragraphs":[]},{"name":"Results","introduction":[{"type":"text","content":[{"type":"plain_text","content":"The parametric material model allows for controlled editing of the object’s appearance. Also the illumination can be adjusted, e.g. for realistic composites. A mesh extraction allows further editing and integration in the standard graphics pipeline including real-time rendering. SHINOBI can help in obtaining relightable 3D assets for e-commerce applications as well as 3D AR and VR for entertainment and education."}]}],"sections":[],"paragraphs":[{"type":"paragraph","name":"Applications","contents":[{"type":"text","content":[{"type":"plain_text","content":"We show a scene featuring objects from the "},{"type":"link_text","content":"NAVI dataset [1]","link":"https://navidataset.github.io/"},{"type":"plain_text","content":" in a new consistent illumination environment as it would be required for AR and VR applications."}]},{"type":"figure","id":"shinobi_scene_relighting"},{"type":"figure","id":"shinobi_scene_material_editing"}]},{"type":"paragraph","name":"Comparison to SAMURAI","contents":[{"type":"text","content":[{"type":"plain_text","content":"We reconstruct PBR material parameters basecolor, metallic and roughness. Compared to "},{"type":"link_text","content":"SAMURAI [2]","link":"https://markboss.me/publication/2022-samurai/"},{"type":"plain_text","content":" high-frequency details are better preserved while optimization time is reduced to roughly a third. Here we show the \"bald eagle\" object from the"},{"type":"link_text","content":"NAVI dataset [1]","link":"https://navidataset.github.io/"},{"type":"plain_text","content":"."}]},{"type":"figure","id":"decomposition"}]},{"type":"paragraph","name":"Reconstructed Assets","contents":[{"type":"text","content":[{"type":"plain_text","content":"Example results from "},{"type":"link_text","content":"NAVI dataset [1]","link":"https://navidataset.github.io/"},{"type":"plain_text","content":". Click on an image for an interactive 3D visualization. Select different environment maps for illumination below."}]}]}]}]},"figures":{"teaser":{"name":"Teaser","type":"video","urls":["videos/shinobi_teaser.mp4"],"styling":{"roundedCorners":true,"objectFit":"contain"}},"pipeline":{"name":"Pipeline","type":"video","urls":["videos/shinobi_overview_animated_website_v002.mp4"],"styling":{"roundedCorners":true,"showControls":true,"objectFit":"contain"}},"decomposition":{"name":"Decomposition","type":"video","urls":["videos/decomposition_comparison_eagle_v001_compressed.mp4"],"styling":{"roundedCorners":true,"showControls":false,"objectFit":"contain"}},"shinobi_scene_relighting":{"name":"Application Relighting","type":"video","urls":["videos/shinobi_integrated_scene_relighting_v05_compressed.mp4"]},"shinobi_scene_material_editing":{"name":"Application Material Editing","type":"swappable_video","urls":["videos/shinobi_material_editing0_v05_compressed.mp4","videos/shinobi_material_editing1_v05_compressed.mp4"]},"overview":{"name":"Overview","type":"youtube","urls":["https://www.youtube.com/embed/iFENQ6AcYd8?si=UCh7iy3EhW3St1OX"]}},"bibliography":{"metadata":{"collection":"bibliography","source":"/Users/jdihlmann/Documents/thesis/parser/../bibliography.bib","created":"2023-07-24T07:16:12.427853","records":1},"records":[{"type":"article","id":"SketchGuidance","citekey":"SketchGuidance","collection":"bibliography","title":"Sketch-Guided Text-to-Image Diffusion Models","year":"2022","url":"https://sketch-guided-diffusion.github.io/","ctitle":"SketchGuidance","abstract":"Text-to-Image models have introduced a remarkable leap in the evolution of machine learning, demonstrating high-quality synthesis of images from a given text-prompt. However, these powerful pretrained models still lack control handles that can guide spatial properties of the synthesized images. In this work, we introduce a universal approach to guide a pretrained text-to-image diffusion model, with a spatial map from another domain (e.g., sketch) during inference time. Unlike previous works, our method does not require to train a dedicated model or a specialized encoder for the task. Our key idea is to train a Latent Guidance Predictor (LGP) - a small, per-pixel, Multi-Layer Perceptron (MLP) that maps latent features of noisy images to spatial maps, where the deep features are extracted from the core Denoising Diffusion Probabilistic Model (DDPM) network. The LGP is trained only on a few thousand images and constitutes a differential guiding map predictor, over which the loss is computed and propagated back to push the intermediate images to agree with the spatial map. The per-pixel training offers flexibility and locality which allows the technique to perform well on out-of-domain sketches, including free-hand style drawings. We take a particular focus on the sketch-to-image translation task, revealing a robust and expressive way to generate images that follow the guidance of a sketch of arbitrary style or domain. Project page: sketch-guided-diffusion.github.io","eprint":"2211.13752","file":":http\\://arxiv.org/pdf/2211.13752v1:PDF","journal":"arXiv preprint arXiv:2211.13752","author":[{"name":"Voynov, Andrey"},{"name":"Aberman, Kfir"},{"name":"Cohen-Or, Daniel"}]}]},"citation":{"type":"misc","name":"engelhardt2023-shinobi","authors":[["Andreas","Engelhardt"],["Amit","Raj"],["Mark","Boss"],["Yunzhi","Zhang"],["Abhishek","Kar"],["Yuanzhen","Li"],["Deqing","Sun"],["Jonathan T.","Barron"],["Hendrik P.A.","Lensch"],["Varun","Jampani"]],"title":"{SHINOBI}: {Sh}ape and {I}llumination using {N}eural {O}bject Decomposition via {B}RDF Optimization {I}n-the-wild","booktitle":"preprint","year":2023,"eprint":"{}","archivePrefix":"arXiv","primaryClass":"cs.CV"},"acknowledgements":{"name":"Acknowledgements","content":[{"type":"text","content":[{"type":"plain_text","content":"This work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 and SFB 1233, TP 02  -  Project number 276693517. It was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A."}]}]},"references":{"name":"References ","content":[{"enum":1,"text":"V. Jampani et al., NAVI: Category-agnostic image collections with high-quality 3D shape and pose annotations, in NeurIPS, 2023."},{"enum":2,"text":"M. Boss et al., SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections, in NeurIPS, 2022."},{"enum":3,"text":"J. Zhang, G. Yang, S. Tulsiani, and D. Ramanan, NeRS: Neural reflectance surfaces for sparse-view 3D reconstruction in the wild, in NeurIPS, 2021."},{"enum":4,"text":"T. Müller, A. Evans, C. Schied, and A. Keller, Instant neural graphics primitives with a multiresolution hash encoding, ACM Trans. Graph., 2022."}]}}},"actionData":null,"errors":null},"future":{"v3_fetcherPersist":false,"v3_relativeSplatPath":false}};</script><script type="module" async="">import "/build/manifest-B5F25652.js";
import * as route0 from "/build/root-WX5QICCQ.js";
import * as route1 from "/build/routes/_index-DE6JVP6Q.js";
window.__remixRouteModules = {"root":route0,"routes/_index":route1};

import("/build/entry.client-J2DQUVRC.js");</script></body></html>