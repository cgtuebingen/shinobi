<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="favicon.ico"/><link rel="stylesheet" href="/build/_assets/app-NRS5MYHR.css"/><link rel="stylesheet" href="https://rsms.me/inter/inter.css"/><title>SHINOBI</title><script src="babylon/babylon.js"></script><script src="babylon/babylonjs.loaders.min.js"></script><script src="babylon/babylon.gui.min.js"></script><script src="https://code.jquery.com/pep/0.4.3/pep.js"></script></head><body style="overscroll-behavior:none"><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H3WNEKDGDG"></script><script async="" id="gtag-init">
								window.dataLayer = window.dataLayer || [];
								function gtag(){dataLayer.push(arguments);}
								gtag('js', new Date());

								gtag('config', 'G-H3WNEKDGDG', {
								page_path: window.location.pathname,
								});
							</script><div class="flex w-full flex-col bg-white dark:bg-black"><div class="mx-auto w-full max-w-4xl px-4 py-12 pb-10 md:px-8"><div class="w-full flex flex-col justify-center items-center space-y-5 pb-10 bg-white rounded-lg"><div class="w-full flex flex-col justify-center items-center space-y-5"><h1 class="text-primary font-sans text-3xl font-bold xl:text-5xl text-center">SHINOBI</h1><h2 class="text-primary font-sans text-xl font-medium xl:text-2xl text-center break-words px-2 max-w-[90%] ">Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild</h2></div><div class="w-full flex flex-wrap justify-center items-center gap-5"><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Andreas Engelhardt</a><a href="https://amitraj93.github.io/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Amit Raj</a><a href="https://markboss.me/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Mark Boss</a><a href="https://cs.stanford.edu/~yzzhang/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Yunzhi Zhang</a><a href="https://abhishekkar.info/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Abhishek Kar</a><a href="https://people.csail.mit.edu/yzli/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Yuanzhen Li</a><a href="https://deqings.github.io/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Deqing Sun</a><a href="https://jonbarron.info/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Jonathan T. Barron</a><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Hendrik P.A. Lensch</a><a href="https://varunjampani.github.io/" class="text-secondary font-sans underline cursor-pointer text-inherit text-center hover:text-primary ">Varun Jampani</a></div><div class="w-full flex flex-row justify-center items-center space-x-10"><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/" class="text-secondary font-sans cursor-pointer hover:text-primary"><img src="images/uni_logo.png" alt="University of Tübingen" class="w-auto h-20"/></a><a href="https://research.google/research-areas/machine-perception/" class="text-secondary font-sans cursor-pointer hover:text-primary"><img src="images/google_logo.png" alt="Google Research" class="w-auto h-20"/></a></div><div class="w-full flex flex-wrap justify-center items-center gap-5"><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer"><img src="icons/arxiv.svg"/><span class="text-m  text-white ">Paper</span></button><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer"><img src="icons/github.svg"/><span class="text-m  text-white ">Code (coming soon)</span></button><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer"><img src="icons/youtube.svg"/><span class="text-m  text-white ">Video</span></button><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer"><img src="icons/results.svg"/><span class="text-m  text-white ">Results</span></button><button class="flex flex-row items-center space-x-1.5 rounded-full pl-1.5 pr-3 py-1 bg-black bg-opacity-90 hover:bg-opacity-80 transition duration-200 ease-in-out cursor-pointer"><img src="icons/bibtex.svg"/><span class="text-m  text-white ">Cite</span></button></div></div><div class="space-y-3 pt-16" id="Abstract"><p class="text-justify block  font-normal "></p> <div class="text-primary dark:text-white"><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:0"><video autoplay="" loop="" playsinline="" muted="" class="" style="object-fit:contain;border-radius:0;transform:scale(undefined)"><source src="videos/shinobi_teaser.mp4" type="video/mp4" data-src="videos/shinobi_teaser.mp4"/></video></div><div class="prose mt-3"><p class="text-justify block font-normal text-primary dark:text-white"><strong>SHINOBI</strong> — <span>reconstructs shape, illumination and materials from in-the-wild image collections.<!-- --> </span></p></div></div></div><p class="text-justify block font-normal text-primary dark:text-white"><span>We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object&#x27;s shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc.<!-- --> </span></p></div><div class="space-y-10 pt-10 text-secondary dark:text-white"><div class="space-y-8"></div></div><div class="space-y-3 pt-16" id="Overview"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Overview</h1><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal text-primary dark:text-white"><span>SHINOBI is a category-agnostic technique to jointly reconstruct 3D shape and material properties of objects from unconstrained in-the-wild image collections. This data regime poses multiple challenges as images are captured in different environments using a variety of devices resulting in varying backgrounds, illumination, camera poses, and intrinsics. Conventional structure-from-motion techniques like COLMAP fail to reconstruct image collections under these challenging conditions. Recent methods like <!-- --> </span><a href="https://markboss.me/publication/2022-samurai/" class="text-primary underline cursor-pointer hover:text-secondary dark:text-white dark:hover:text-white dark:hover:text-opacity-80">SAMURAI [2]</a><span> and <!-- --> </span><a href="https://jasonyzhang.com/ners/" class="text-primary underline cursor-pointer hover:text-secondary dark:text-white dark:hover:text-white dark:hover:text-opacity-80">NeRS [3]</a><span> can be initialized from very coarse view directions but still yield low quality reconstructions for many challenging scenes. Additionally, optimization takes more than 12 hours in the case of SAMURAI. In contrast, we propose a pipeline based on <!-- --> </span><a href="https://nvlabs.github.io/instant-ngp/" class="text-primary underline cursor-pointer hover:text-secondary dark:text-white dark:hover:text-white dark:hover:text-opacity-80">multiresolution hash grids [4]</a><span> which allows us to process more rays in a shorter time. Using this advantage we are able to improve reconstruction quality while still keeping a competitive optimization time (~4 hours). However, naive integration of multi-resolution hash grids is not well suited to camera pose estimation due to discontinuities in the gradients with respect to the input positions. We propose several components that work together to stabilize the camera pose optimization and encourage sharp features:<!-- --> </span></p><ul class="list-disc list-outside ml-5 text-justify block font-normal text-primary dark:text-white"><li><span>Hybrid Multiresolution Hash Encoding with resolution level annealing<!-- --> </span></li><li><span>Optimized camera parameterization and constraint camera multiplex using a projection based loss over all camera proposals for a given view<!-- --> </span></li><li><span>Per-view importance weighting to leverage the important observation that some views are more useful for optimization than others<!-- --> </span></li><li><span>Patch-based alignment losses to aid in camera alignment and reconstruction of high-frequency details<!-- --> </span></li></ul><div class="text-primary dark:text-white"><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-bottom:56.25%;overflow:hidden;height:0"><iframe width="853" height="480" class="absolute top-0 left-0 w-full h-full" src="https://www.youtube.com/embed/bn7L-va0Y78?si=aw5122sxjJXcCpl1" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameBorder="0"></iframe></div></div></div></div><div class="space-y-10 pt-10 text-secondary dark:text-white"><div class="space-y-8"></div></div><div class="space-y-3 pt-16" id="Method"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Method</h1><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal text-primary dark:text-white"><span>In the figure below we visualize the SHINOBI optimization pipeline. Two resolution annealed encoding branches, the multiresolution hash grid and the Fourier embedding are used to learn a neural volume conditioned on the input coordinates and illumination. Our patch-based losses and regularization scheme enables robust optimization of camera parameters jointly with the shape, material and per image illumination.<!-- --> </span></p><div class="text-primary dark:text-white"><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:0"><video autoplay="" loop="" controls="" playsinline="" muted="" class="" style="object-fit:contain;border-radius:0;transform:scale(undefined)"><source src="videos/shinobi_overview_animated_website_v002.mp4" type="video/mp4" data-src="videos/shinobi_overview_animated_website_v002.mp4"/></video></div></div></div></div><div class="space-y-10 pt-10 text-secondary dark:text-white"><div class="space-y-8"></div></div><div class="space-y-3 pt-16" id="Results"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Results</h1><p class="text-justify block  font-normal "></p> <p class="text-justify block font-normal text-primary dark:text-white"><span>The parametric material model allows for controlled editing of the object’s appearance. Also the illumination can be adjusted, e.g. for realistic composites. A mesh extraction allows further editing and integration in the standard graphics pipeline including real-time rendering. SHINOBI can help in obtaining relightable 3D assets for e-commerce applications as well as 3D AR and VR for entertainment and education.<!-- --> </span></p></div><div class="space-y-10 pt-10 text-secondary dark:text-white"><div class="space-y-8"><div class="prose mt-3"><p class="text-justify block font-normal text-primary dark:text-white"><strong>Applications</strong> — <span>We show a scene featuring objects from the <!-- --> </span><a href="https://navidataset.github.io/" class="text-primary underline cursor-pointer hover:text-secondary dark:text-white dark:hover:text-white dark:hover:text-opacity-80">NAVI dataset [1]</a><span> in a new consistent illumination environment as it would be required for AR and VR applications.<!-- --> </span></p><div class="text-primary dark:text-white"><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:56.25%"><video autoplay="" loop="" playsinline="" muted="" class="absolute top-0 left-0 w-full h-full" style="object-fit:cover;border-radius:0.5rem;transform:scale(1)"><source src="videos/shinobi_integrated_scene_relighting_v05_compressed.mp4" type="video/mp4" data-src="videos/shinobi_integrated_scene_relighting_v05_compressed.mp4"/></video></div></div></div><div class="text-primary dark:text-white"><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:56.25%"><video autoplay="" loop="" playsinline="" muted="" class="absolute top-0 left-0 w-full h-full" style="object-fit:cover;transition:none;opacity:1;z-index:0;border-radius:0.5rem"><source src="videos/shinobi_material_editing0_v05_compressed.mp4" type="video/mp4"/></video><video autoplay="" loop="" playsinline="" muted="" class="absolute top-0 left-0 w-full h-full" style="object-fit:cover;transition:opacity 500ms ease-in-out;opacity:0;z-index:1;border-radius:0.5rem"><source src="videos/shinobi_material_editing1_v05_compressed.mp4" type="video/mp4"/></video><div class="absolute bottom-2 right-2 flex flex-row space-x-1.5"><video autoplay="" loop="" playsinline="" muted="" class="w-16 h-16 cursor-pointer shadow-lg border-2" style="object-fit:cover;filter:grayscale(100%);border-color:#fff;z-index:2;border-radius:0.5rem"><source src="videos/shinobi_material_editing0_v05_compressed.mp4" type="video/mp4"/></video><video autoplay="" loop="" playsinline="" muted="" class="w-16 h-16 cursor-pointer shadow-lg border-2" style="object-fit:cover;filter:none;border-color:rgba(255, 255, 255, 0.5);z-index:2;border-radius:0.5rem"><source src="videos/shinobi_material_editing1_v05_compressed.mp4" type="video/mp4"/></video></div></div></div></div></div><div class="prose mt-3"><p class="text-justify block font-normal text-primary dark:text-white"><strong>Comparison to SAMURAI</strong> — <span>We reconstruct PBR material parameters basecolor, metallic and roughness. Compared to <!-- --> </span><a href="https://markboss.me/publication/2022-samurai/" class="text-primary underline cursor-pointer hover:text-secondary dark:text-white dark:hover:text-white dark:hover:text-opacity-80">SAMURAI [2]</a><span> high-frequency details are better preserved while optimization time is reduced to roughly a third. Here we show the &quot;bald eagle&quot; object from the<!-- --> </span><a href="https://navidataset.github.io/" class="text-primary underline cursor-pointer hover:text-secondary dark:text-white dark:hover:text-white dark:hover:text-opacity-80">NAVI dataset [1]</a><span>.<!-- --> </span></p><div class="text-primary dark:text-white"><div class="space-y-3 pb-10" style="padding-top:1rem;filter:none"><div class="relative w-full" style="padding-top:0"><video autoplay="" loop="" playsinline="" muted="" class="" style="object-fit:contain;border-radius:0;transform:scale(undefined)"><source src="videos/decomposition_comparison_eagle_v001_compressed.mp4" type="video/mp4" data-src="videos/decomposition_comparison_eagle_v001_compressed.mp4"/></video></div></div></div></div><div class="prose mt-3"><p class="text-justify block font-normal text-primary dark:text-white"><strong>Reconstructed Assets</strong> — <span>Example results from <!-- --> </span><a href="https://navidataset.github.io/" class="text-primary underline cursor-pointer hover:text-secondary dark:text-white dark:hover:text-white dark:hover:text-opacity-80">NAVI dataset [1]</a><span>. Click on an image for an interactive 3D visualization. Select different environment maps for illumination below.<!-- --> </span></p></div></div></div><div class="flex flex-col justify-center items-center gap-2"><div class="w-full h-20 grid grid-cols-fill gap-2 grid-flow-col "><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/tractor.jpg" alt="tractor" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/remotecar.jpg" alt="remotecar" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/keywest.jpg" alt="keywest" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/lion.jpg" alt="lion" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/baldeagle.jpg" alt="baldeagle" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/fireengine.jpg" alt="fireengine" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/pumpkin.jpg" alt="pumpkin" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/dino_5.jpg" alt="dino_5" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/hutmushroom.jpg" alt="hutmushroom" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/models/colored_box.jpg" alt="colored_box" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:1 / 1"/></button></div><div class="relative w-full bg-slate-200 rounded-lg" style="height:30rem;overflow:hidden"><div class="absolute top-0 left-0 w-full h-full " style="filter:blur(100px)"><img class="w-full h-full object-cover rounded-lg" src="/hdris/lebombo.jpg" alt="lebombo"/></div><div style="position:relative;width:100%;height:100%;overflow:hidden;pointer-events:auto" class="absolute top-0 left-0 w-full h-full"><div style="width:100%;height:100%"><canvas style="display:block"></canvas></div></div></div><div class="w-full h-20 grid grid-cols-fill gap-2 grid-flow-col "><button class="relative w-auto h-auto hover:opacity-80"><img src="/hdris/lebombo.jpg" alt="lebombo" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:2 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/hdris/photo_studio.jpg" alt="photo_studio" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:2 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/hdris/forest_slope.jpg" alt="forest_slope" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:2 / 1"/></button><button class="relative w-auto h-auto hover:opacity-80"><img src="/hdris/urban_alley.jpg" alt="urban_alley" class="w-full h-full object-cover rounded-lg" style="aspect-ratio:2 / 1"/></button></div></div><div class="space-y-3 pt-16" id="Citation"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Citation</h1><div class="group relative rounded-lg bg-black bg-opacity-5 hover:bg-opacity-10 transition duration-200 ease-in-out p-2 dark:bg-white dark:bg-opacity-5 dark:hover:bg-opacity-10"><button class="opacity-0 group-hover:opacity-100 absolute top-2 right-2 p-2 rounded-lg bg-black text-white transition duration-200 ease-in-out cursor-pointer z-10 hover:bg-opacity-80 dark:bg-white dark:bg-opacity-5 dark:hover:bg-opacity-80 dark:text-black"><img src="/icons/copy.svg" class="w-4 h-4" alt="Copy to Clipboard"/></button><div class="text-justify grid grid-cols-[auto_1fr] text-primary dark:text-white"><span>@<!-- -->misc<!-- -->{</span><span>engelhardt2023-shinobi<!-- -->,</span> <span class="pl-12">author =</span><span>{<!-- -->Engelhardt, Andreas and <!-- -->Raj, Amit and <!-- -->Boss, Mark and <!-- -->Zhang, Yunzhi and <!-- -->Kar, Abhishek and <!-- -->Li, Yuanzhen and <!-- -->Sun, Deqing and <!-- -->Barron, Jonathan T. and <!-- -->Lensch, Hendrik P.A. and <!-- -->Jampani, Varun<!-- -->},</span><span class="pl-12">title =</span><span>{<!-- -->{{SHINOBI}: {Sh}ape and {I}llumination using {N}eural {O}bject Decomposition via {B}RDF Optimization {I}n-the-wild}<!-- -->},</span><span class="pl-12">booktitle =</span><span>{<!-- -->{preprint}<!-- -->},</span><span class="pl-12">year =</span><span>{<!-- -->2023<!-- -->}</span><span>}</span></div></div></div><div class="space-y-3 pt-16" id="Acknowledgements"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">Acknowledgements</h1><p class="text-primary text-justify block font-normal dark:text-white"></p> <p class="text-justify block font-normal text-primary dark:text-white"><span>This work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 and SFB 1233, TP 02  -  Project number 276693517. It was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A.<!-- --> </span></p></div><div class="space-y-3 pt-16"><h1 class="text-primary font-sans text-xl font-bold dark:text-white">References</h1><p class="text-justify block font-normal "></p> <p class="text-primary text-justify block font-normal dark:text-white"> <!-- -->[1]<!-- --> <!-- -->V. Jampani et al., NAVI: Category-agnostic image collections with high-quality 3D shape and pose annotations, in NeurIPS, 2023.<!-- --> </p><p class="text-primary text-justify block font-normal dark:text-white"> <!-- -->[2]<!-- --> <!-- -->M. Boss et al., SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections, in NeurIPS, 2022.<!-- --> </p><p class="text-primary text-justify block font-normal dark:text-white"> <!-- -->[3]<!-- --> <!-- -->J. Zhang, G. Yang, S. Tulsiani, and D. Ramanan, NeRS: Neural reflectance surfaces for sparse-view 3D reconstruction in the wild, in NeurIPS, 2021.<!-- --> </p><p class="text-primary text-justify block font-normal dark:text-white"> <!-- -->[4]<!-- --> <!-- -->T. Müller, A. Evans, C. Schied, and A. Keller, Instant neural graphics primitives with a multiresolution hash encoding, ACM Trans. Graph.,, 2022.<!-- --> </p></div></div></div><script>((STORAGE_KEY, restoreKey) => {
    if (!window.history.state || !window.history.state.key) {
      let key = Math.random().toString(32).slice(2);
      window.history.replaceState({
        key
      }, "");
    }
    try {
      let positions = JSON.parse(sessionStorage.getItem(STORAGE_KEY) || "{}");
      let storedY = positions[restoreKey || window.history.state.key];
      if (typeof storedY === "number") {
        window.scrollTo(0, storedY);
      }
    } catch (error) {
      console.error(error);
      sessionStorage.removeItem(STORAGE_KEY);
    }
  })("positions", null)</script><link rel="modulepreload" href="/build/manifest-1E62CCD7.js"/><link rel="modulepreload" href="/build/entry.client-J2DQUVRC.js"/><link rel="modulepreload" href="/build/_shared/chunk-D7TEDUQN.js"/><link rel="modulepreload" href="/build/_shared/chunk-3Z32TANI.js"/><link rel="modulepreload" href="/build/_shared/chunk-T36URGAI.js"/><link rel="modulepreload" href="/build/_shared/chunk-KPWQHS6G.js"/><link rel="modulepreload" href="/build/root-IMG7T2SC.js"/><link rel="modulepreload" href="/build/_shared/chunk-4Z5PEYEG.js"/><link rel="modulepreload" href="/build/_shared/chunk-S4FTTNEK.js"/><link rel="modulepreload" href="/build/routes/_index-RIEQUG2X.js"/><script>window.__remixContext = {"url":"/","state":{"loaderData":{"root":{"title":{"title":"SHINOBI","subtitle":"Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild"}},"routes/_index":{"title":{"title":"SHINOBI","subtitle":"Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild"},"authors":[{"name":"Andreas Engelhardt","link":"https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/"},{"name":"Amit Raj","link":"https://amitraj93.github.io/"},{"name":"Mark Boss","link":"https://markboss.me/"},{"name":"Yunzhi Zhang","link":"https://cs.stanford.edu/~yzzhang/"},{"name":"Abhishek Kar","link":"https://abhishekkar.info/"},{"name":"Yuanzhen Li","link":"https://people.csail.mit.edu/yzli/"},{"name":"Deqing Sun","link":"https://deqings.github.io/"},{"name":"Jonathan T. Barron","link":"https://jonbarron.info/"},{"name":"Hendrik P.A. Lensch","link":"https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/"},{"name":"Varun Jampani","link":"https://varunjampani.github.io/"}],"institutions":[{"name":"University of Tübingen","link":"https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/","image":"images/uni_logo.png"},{"name":"Google Research","link":"https://research.google/research-areas/machine-perception/","image":"images/google_logo.png"}],"links":[{"icon":"arxiv","name":"Paper","link":"https://arxiv.org"},{"icon":"github","name":"Code (coming soon)","link":""},{"icon":"youtube","name":"Video","link":"https://youtu.be/bn7L-va0Y78"},{"icon":"results","name":"Results","link":"#Results"},{"icon":"bibtex","name":"Cite","link":"#Citation"}],"document":{"chapters":[{"name":"Abstract","styling":{"hideHeading":true},"introduction":[{"type":"figure","id":"teaser"},{"type":"text","content":[{"type":"plain_text","content":"We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc."}]}],"sections":[],"paragraphs":[]},{"name":"Overview","introduction":[{"type":"text","content":[{"type":"plain_text","content":"SHINOBI is a category-agnostic technique to jointly reconstruct 3D shape and material properties of objects from unconstrained in-the-wild image collections. This data regime poses multiple challenges as images are captured in different environments using a variety of devices resulting in varying backgrounds, illumination, camera poses, and intrinsics. Conventional structure-from-motion techniques like COLMAP fail to reconstruct image collections under these challenging conditions. Recent methods like "},{"type":"link_text","content":"SAMURAI [2]","link":"https://markboss.me/publication/2022-samurai/"},{"type":"plain_text","content":" and "},{"type":"link_text","content":"NeRS [3]","link":"https://jasonyzhang.com/ners/"},{"type":"plain_text","content":" can be initialized from very coarse view directions but still yield low quality reconstructions for many challenging scenes. Additionally, optimization takes more than 12 hours in the case of SAMURAI. In contrast, we propose a pipeline based on "},{"type":"link_text","content":"multiresolution hash grids [4]","link":"https://nvlabs.github.io/instant-ngp/"},{"type":"plain_text","content":" which allows us to process more rays in a shorter time. Using this advantage we are able to improve reconstruction quality while still keeping a competitive optimization time (~4 hours). However, naive integration of multi-resolution hash grids is not well suited to camera pose estimation due to discontinuities in the gradients with respect to the input positions. We propose several components that work together to stabilize the camera pose optimization and encourage sharp features:"}]},{"type":"list","content":[{"type":"text","content":[{"type":"plain_text","content":"Hybrid Multiresolution Hash Encoding with resolution level annealing"}]},{"type":"text","content":[{"type":"plain_text","content":"Optimized camera parameterization and constraint camera multiplex using a projection based loss over all camera proposals for a given view"}]},{"type":"text","content":[{"type":"plain_text","content":"Per-view importance weighting to leverage the important observation that some views are more useful for optimization than others"}]},{"type":"text","content":[{"type":"plain_text","content":"Patch-based alignment losses to aid in camera alignment and reconstruction of high-frequency details"}]}]},{"type":"figure","id":"overview"}],"sections":[],"paragraphs":[]},{"name":"Method","introduction":[{"type":"text","content":[{"type":"plain_text","content":"In the figure below we visualize the SHINOBI optimization pipeline. Two resolution annealed encoding branches, the multiresolution hash grid and the Fourier embedding are used to learn a neural volume conditioned on the input coordinates and illumination. Our patch-based losses and regularization scheme enables robust optimization of camera parameters jointly with the shape, material and per image illumination."}]},{"type":"figure","id":"pipeline"}],"sections":[],"paragraphs":[]},{"name":"Results","introduction":[{"type":"text","content":[{"type":"plain_text","content":"The parametric material model allows for controlled editing of the object’s appearance. Also the illumination can be adjusted, e.g. for realistic composites. A mesh extraction allows further editing and integration in the standard graphics pipeline including real-time rendering. SHINOBI can help in obtaining relightable 3D assets for e-commerce applications as well as 3D AR and VR for entertainment and education."}]}],"sections":[],"paragraphs":[{"type":"paragraph","name":"Applications","contents":[{"type":"text","content":[{"type":"plain_text","content":"We show a scene featuring objects from the "},{"type":"link_text","content":"NAVI dataset [1]","link":"https://navidataset.github.io/"},{"type":"plain_text","content":" in a new consistent illumination environment as it would be required for AR and VR applications."}]},{"type":"figure","id":"shinobi_scene_relighting"},{"type":"figure","id":"shinobi_scene_material_editing"}]},{"type":"paragraph","name":"Comparison to SAMURAI","contents":[{"type":"text","content":[{"type":"plain_text","content":"We reconstruct PBR material parameters basecolor, metallic and roughness. Compared to "},{"type":"link_text","content":"SAMURAI [2]","link":"https://markboss.me/publication/2022-samurai/"},{"type":"plain_text","content":" high-frequency details are better preserved while optimization time is reduced to roughly a third. Here we show the \"bald eagle\" object from the"},{"type":"link_text","content":"NAVI dataset [1]","link":"https://navidataset.github.io/"},{"type":"plain_text","content":"."}]},{"type":"figure","id":"decomposition"}]},{"type":"paragraph","name":"Reconstructed Assets","contents":[{"type":"text","content":[{"type":"plain_text","content":"Example results from "},{"type":"link_text","content":"NAVI dataset [1]","link":"https://navidataset.github.io/"},{"type":"plain_text","content":". Click on an image for an interactive 3D visualization. Select different environment maps for illumination below."}]}]}]}]},"figures":{"teaser":{"name":"Teaser","type":"video","urls":["videos/shinobi_teaser.mp4"],"styling":{"roundedCorners":false,"objectFit":"contain"},"captions":[{"type":"paragraph","name":"SHINOBI","contents":[{"type":"text","content":[{"type":"plain_text","content":"reconstructs shape, illumination and materials from in-the-wild image collections."}]}]}]},"pipeline":{"name":"Pipeline","type":"video","urls":["videos/shinobi_overview_animated_website_v002.mp4"],"styling":{"roundedCorners":false,"showControls":true,"objectFit":"contain"}},"decomposition":{"name":"Decomposition","type":"video","urls":["videos/decomposition_comparison_eagle_v001_compressed.mp4"],"styling":{"roundedCorners":false,"showControls":false,"objectFit":"contain"}},"shinobi_scene_relighting":{"name":"Application Relighting","type":"video","urls":["videos/shinobi_integrated_scene_relighting_v05_compressed.mp4"]},"shinobi_scene_material_editing":{"name":"Application Material Editing","type":"swappable_video","urls":["videos/shinobi_material_editing0_v05_compressed.mp4","videos/shinobi_material_editing1_v05_compressed.mp4"]},"overview":{"name":"Overview","type":"youtube","urls":["https://www.youtube.com/embed/bn7L-va0Y78?si=aw5122sxjJXcCpl1"]}},"bibliography":{"metadata":{"collection":"bibliography","source":"/Users/jdihlmann/Documents/thesis/parser/../bibliography.bib","created":"2023-07-24T07:16:12.427853","records":1},"records":[{"type":"article","id":"SketchGuidance","citekey":"SketchGuidance","collection":"bibliography","title":"Sketch-Guided Text-to-Image Diffusion Models","year":"2022","url":"https://sketch-guided-diffusion.github.io/","ctitle":"SketchGuidance","abstract":"Text-to-Image models have introduced a remarkable leap in the evolution of machine learning, demonstrating high-quality synthesis of images from a given text-prompt. However, these powerful pretrained models still lack control handles that can guide spatial properties of the synthesized images. In this work, we introduce a universal approach to guide a pretrained text-to-image diffusion model, with a spatial map from another domain (e.g., sketch) during inference time. Unlike previous works, our method does not require to train a dedicated model or a specialized encoder for the task. Our key idea is to train a Latent Guidance Predictor (LGP) - a small, per-pixel, Multi-Layer Perceptron (MLP) that maps latent features of noisy images to spatial maps, where the deep features are extracted from the core Denoising Diffusion Probabilistic Model (DDPM) network. The LGP is trained only on a few thousand images and constitutes a differential guiding map predictor, over which the loss is computed and propagated back to push the intermediate images to agree with the spatial map. The per-pixel training offers flexibility and locality which allows the technique to perform well on out-of-domain sketches, including free-hand style drawings. We take a particular focus on the sketch-to-image translation task, revealing a robust and expressive way to generate images that follow the guidance of a sketch of arbitrary style or domain. Project page: sketch-guided-diffusion.github.io","eprint":"2211.13752","file":":http\\://arxiv.org/pdf/2211.13752v1:PDF","journal":"arXiv preprint arXiv:2211.13752","author":[{"name":"Voynov, Andrey"},{"name":"Aberman, Kfir"},{"name":"Cohen-Or, Daniel"}]}]},"citation":{"type":"misc","name":"engelhardt2023-shinobi","authors":[["Andreas","Engelhardt"],["Amit","Raj"],["Mark","Boss"],["Yunzhi","Zhang"],["Abhishek","Kar"],["Yuanzhen","Li"],["Deqing","Sun"],["Jonathan T.","Barron"],["Hendrik P.A.","Lensch"],["Varun","Jampani"]],"title":"{{SHINOBI}: {Sh}ape and {I}llumination using {N}eural {O}bject Decomposition via {B}RDF Optimization {I}n-the-wild}","booktitle":"{preprint}","year":2023,"eprint":"{}","archivePrefix":"{arXiv}","primaryClass":"{cs.CV}"},"acknowledgements":{"name":"Acknowledgements","content":[{"type":"text","content":[{"type":"plain_text","content":"This work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 and SFB 1233, TP 02  -  Project number 276693517. It was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A."}]}]},"references":{"name":"References ","content":[{"enum":1,"text":"V. Jampani et al., NAVI: Category-agnostic image collections with high-quality 3D shape and pose annotations, in NeurIPS, 2023."},{"enum":2,"text":"M. Boss et al., SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections, in NeurIPS, 2022."},{"enum":3,"text":"J. Zhang, G. Yang, S. Tulsiani, and D. Ramanan, NeRS: Neural reflectance surfaces for sparse-view 3D reconstruction in the wild, in NeurIPS, 2021."},{"enum":4,"text":"T. Müller, A. Evans, C. Schied, and A. Keller, Instant neural graphics primitives with a multiresolution hash encoding, ACM Trans. Graph.,, 2022."}]}}},"actionData":null,"errors":null},"future":{"v3_fetcherPersist":false,"v3_relativeSplatPath":false}};</script><script type="module" async="">import "/build/manifest-1E62CCD7.js";
import * as route0 from "/build/root-IMG7T2SC.js";
import * as route1 from "/build/routes/_index-RIEQUG2X.js";
window.__remixRouteModules = {"root":route0,"routes/_index":route1};

import("/build/entry.client-J2DQUVRC.js");</script></body></html>